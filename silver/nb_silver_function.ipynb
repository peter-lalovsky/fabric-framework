{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a5622-b6de-4491-b401-40e97331fd97",
   "metadata": {
    "editable": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"defaultLakehouse\": {\"name\": \"lh_cfg\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5baf2c-ff0b-4725-9bae-eac0578f6a2a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# PL: Please turn off your \"AutoSave\" (Edit --> AutoSave --> Off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ffcef-20b0-4fbd-ae7f-c58687e2d974",
   "metadata": {
    "editable": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "# kill me\n",
    "\n",
    "# Temp cell for emulating the parameters, passed by pl_master --> nb_master\n",
    "\n",
    "is_debug         = False\n",
    "do_test_function = False\n",
    "called_from = \"bronze\" # (silver, gold, power_bi, etc.)\n",
    "\n",
    "import delta.tables as dlt\n",
    "import json as j\n",
    "from types import SimpleNamespace as sn\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql import types as st\n",
    "from inspect import stack as stk\n",
    "import time as t\n",
    "import datetime as dt\n",
    "import pytz as pt\n",
    "import re\n",
    "import unicodedata as ucd\n",
    "import pandas as pd\n",
    "import requests as req\n",
    "if is_debug:\n",
    "    from pyspark.sql import SparkSession as ss\n",
    "\n",
    "global_parameter = \"\"\"\n",
    "{\n",
    "    \"days_to_keep_log\": 365,\n",
    "    \"send_success_email\": \"True\",\n",
    "    \"send_error_email\": \"True\",\n",
    "    \"email_on_success\": \"example@example.com\",\n",
    "    \"email_on_error\": \"example@example.com\",\n",
    "    \"time_zone_nb\": \"Canada/Eastern\",\n",
    "    \"time_zone_pl\": \"Eastern Standard Time\",\n",
    "    \"mssql_isolation_level\": \"READ_UNCOMMITTED\",\n",
    "    \"projet_documentation_data_file_name\": \"configuration\",\n",
    "    \"abfs_path_lh_log\": \"abfss://abc@onelake.dfs.fabric.microsoft.com/abc\",\n",
    "    \"abfs_path_lh_bronze\": \"abfss://abc@onelake.dfs.fabric.microsoft.com/abc\",\n",
    "    \"abfs_path_lh_silver\": \"abfss://abc@onelake.dfs.fabric.microsoft.com/1abc\",\n",
    "    \"abfs_path_lh_cfg\": \"abfss://abc@onelake.dfs.fabric.microsoft.com/abc\",\n",
    "    \"now_dt\": \\\"2024-05-07 14:09:09.367\",\n",
    "    \"extraction_timestamp\": \"20240507_140909367\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da754a-90fd-4006-a2f2-07308794fd50",
   "metadata": {
    "editable": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "# kill me\n",
    "\n",
    "# Create local variables from the parameters, paseed by \"pl_master\"\n",
    "global_parameter = j.loads(global_parameter) # Convert string to mapping\n",
    "global_parameter = sn(**global_parameter) # Create variables from mapping\n",
    "\n",
    "if is_debug:\n",
    "    print(f\"----- Test global_parameter - Start -----\")\n",
    "    print(f\"global_parameter.days_to_keep_log:                    {global_parameter.days_to_keep_log}\")\n",
    "    print(f\"global_parameter.send_success_email:                  {global_parameter.send_success_email}\")\n",
    "    print(f\"global_parameter.send_error_email:                    {global_parameter.send_error_email}\")\n",
    "    print(f\"global_parameter.email_on_success:                    {global_parameter.email_on_success}\")\n",
    "    print(f\"global_parameter.email_on_error:                      {global_parameter.email_on_error}\")\n",
    "    print(f\"global_parameter.time_zone_nb:                        {global_parameter.time_zone_nb}\")\n",
    "    print(f\"global_parameter.time_zone_pl:                        {global_parameter.time_zone_pl}\")\n",
    "    print(f\"global_parameter.mssql_isolation_level:               {global_parameter.mssql_isolation_level}\")\n",
    "    print(f\"global_parameter.projet_documentation_data_file_name: {global_parameter.projet_documentation_data_file_name}\")\n",
    "    print(f\"global_parameter.abfs_path_lh_log:                    {global_parameter.abfs_path_lh_log}\")\n",
    "    print(f\"global_parameter.abfs_path_lh_bronze:                 {global_parameter.abfs_path_lh_bronze}\")\n",
    "    print(f\"global_parameter.abfs_path_lh_silver:                 {global_parameter.abfs_path_lh_silver}\")\n",
    "    print(f\"global_parameter.abfs_path_lh_cfg:                    {global_parameter.abfs_path_lh_cfg}\")\n",
    "    print(f\"global_parameter.now_dt:                              {global_parameter.now_dt}\")\n",
    "    print(f\"global_parameter.extraction_timestamp:                {global_parameter.extraction_timestamp}\")\n",
    "    print(f\"----- Test global_parameter - End -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce5bb0-b681-4a0f-b7aa-3a0af5cfda5c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Print current session info\n",
    "if is_debug:\n",
    "    print(f\"----- Current session info - Start -----\")\n",
    "    display(ss.getActiveSession())\n",
    "    print(f\"----- Current session info - End -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2bc5d-7941-4dad-ad0d-1249329155e6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%run nb_system_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e4ed26-ba7e-4cf4-9f96-19aaa27e5e42",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# kill me\n",
    "\n",
    "# Overwrite global_parameters - now_dt and extraction_timestamp\n",
    "\n",
    "now_dt               = fn_get_now(\"datetime\")[1]\n",
    "extraction_timestamp = fn_get_now(\"string\")[1]\n",
    "\n",
    "global_parameter.now_dt               = now_dt\n",
    "global_parameter.extraction_timestamp = extraction_timestamp\n",
    "\n",
    "print(global_parameter.now_dt)\n",
    "print(global_parameter.extraction_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb007502-dbc1-4e18-8a4c-b3041bf280a7",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c313a68-d151-410b-9608-4dfa5602af4e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Add metadata columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16665c-46be-4782-b496-f0ddfb855838",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# From table \"lh_cfg.metadata_column\"\n",
    "def fn_add_metadata_column(\n",
    "    sdf,\n",
    "    dict_parameter\n",
    "):\n",
    "    fn_name        = stk()[0][3]\n",
    "    caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "    if is_debug:\n",
    "        par = {}\n",
    "\n",
    "    try:\n",
    "        # PL: Add warnings where needed\n",
    "\n",
    "        # Collect metadata columns\n",
    "        sdf_metadata_column = fn_execute_spark_sql(f\"\"\"\n",
    "            SELECT DISTINCT `name`\n",
    "            FROM `metadata_column`\n",
    "            WHERE\n",
    "                `technology`    = '{dict_parameter[\"technology\"]}'\n",
    "                AND `frequency` = '{dict_parameter[\"frequency\"]}';\n",
    "        \"\"\")[1]\n",
    "\n",
    "        list_metadata_column = []\n",
    "        for row in sdf_metadata_column.collect():\n",
    "            list_metadata_column.append(row.name)\n",
    "\n",
    "        # Add metadatacolumn \"fa_date_extracted\"\n",
    "        if \"fa_date_extracted\" in list_metadata_column:\n",
    "            sdf = sdf.withColumn(\"fa_date_extracted\", sf.lit(global_parameter.now_dt).cast(\"Timestamp\"))\n",
    "\n",
    "        # Add metadatacolumn \"fa_server_name\"\n",
    "        if \"fa_server_name\" in list_metadata_column:\n",
    "            sdf = sdf.withColumn(\"fa_server_name\", sf.lit(dict_parameter[\"server_name\"]))\n",
    "\n",
    "        # Add metadatacolumn \"fa_database_name\"\n",
    "        if \"fa_database_name\" in list_metadata_column:\n",
    "            sdf = sdf.withColumn(\"fa_database_name\", sf.lit(dict_parameter[\"database_name\"]))\n",
    "\n",
    "        # Add metadatacolumn \"fa_schema_name\"\n",
    "        if \"fa_schema_name\" in list_metadata_column:\n",
    "            sdf = sdf.withColumn(\"fa_schema_name\", sf.lit(dict_parameter[\"schema_name\"]))\n",
    "\n",
    "        # Add metadatacolumn \"fa_table_name\"\n",
    "        if \"fa_table_name\" in list_metadata_column:\n",
    "            sdf = sdf.withColumn(\"fa_table_name\", sf.lit(dict_parameter[\"table_name\"]))\n",
    "\n",
    "        sdf_count = sdf.count()\n",
    "\n",
    "        if is_debug:\n",
    "            par[\"locals\"] = locals()\n",
    "            par[\"sdf\"] = sdf.show(truncate = False, n = 5)\n",
    "            fn_print_debug_info(\"Success\", fn_name, par)\n",
    "\n",
    "        return (True, sdf, sdf_count)\n",
    "    except Exception as ex:\n",
    "        ex = str(ex)\n",
    "\n",
    "        if is_debug:\n",
    "            par[\"locals\"] = locals()\n",
    "            par[\"exception\"] = ex\n",
    "            fn_print_debug_info(\"Error\", fn_name, par)\n",
    "\n",
    "        sdf            = str(sdf)\n",
    "        dict_parameter = j.dumps(dict_parameter)\n",
    "        if is_debug:\n",
    "            par = str(par)\n",
    "        fn_lh_log_log_insert(global_parameter.extraction_timestamp, \"Silver\", fn_name, str(locals()), \"Danger\", ex)\n",
    "\n",
    "        return (False, ex)\n",
    "\n",
    "# Test\n",
    "if is_debug == True and do_test_function == True:\n",
    "    print(f\"\\n----- Test fn_add_metadata_column() - Start -----\\n\")\n",
    "    sdf = spark.createDataFrame(\n",
    "        [(1, \"column_1\", \"String\", \"alias_col_1\"),\n",
    "         (2, \"column_2\", \"Integer\", \"alias_col_2\")],\n",
    "        [\"sequence\", \"column_name\", \"data_type\", \"alias_name\"])\n",
    "    dict_parameter = {\n",
    "        \"technology\": \"Excel\",\n",
    "        \"frequency\": \"Daily\",\n",
    "        \"server_name\": \"Reference\",\n",
    "        \"database_name\": \"Data\",\n",
    "        \"schema_name\": \"Rapport_sur les coûts de dist. des addendas\",\n",
    "        \"table_name\": \"Rapport\"\n",
    "    }\n",
    "    print(f\"Input value (sdf) type:            {type(sdf)}\")\n",
    "    print(f\"Input value (sdf):\")\n",
    "    sdf.show(truncate = False, n = 5)\n",
    "    print(f\"Input value (dict_parameter) type: {type(dict_parameter)}\")\n",
    "    print(f\"Input value (dict_parameter):      {dict_parameter}\")\n",
    "    print(f\"\\n# Execution (try) - Start #\")\n",
    "    time_start = dt.datetime.now()\n",
    "    rv_try = fn_add_metadata_column(sdf, dict_parameter)\n",
    "    print(f\"# Execution (try) - End (duration: {(dt.datetime.now() - time_start)}) #\\n\")\n",
    "    print(f\"Return value:               {rv_try}\")\n",
    "    print(f\"Return value (status) type: {type(rv_try[0])}\")\n",
    "    print(f\"Return value (status):      {rv_try[0]}\")\n",
    "    print(f\"Return value (sdf) type:    {type(rv_try[1])}\")\n",
    "    print(f\"Return value (sdf) schema:\")\n",
    "    rv_try[1].printSchema()\n",
    "    print(f\"Return value (sdf):\")\n",
    "    rv_try[1].show(truncate = False, n = 5)\n",
    "    print(f\"Return value (sdf_count) type: {type(rv_try[2])}\")\n",
    "    print(f\"Return value (sdf_count):      {rv_try[2]}\")\n",
    "    print(\"\\n--------------------\\n\")\n",
    "    print(f\"\\n# Execution (except) - Start #\")\n",
    "    dict_parameter = {\n",
    "        \"technolog\": \"Excel\",\n",
    "        \"frequenc\": \"Daily\",\n",
    "        \"server_nam\": \"Reference\",\n",
    "        \"database_nam\": \"Data\",\n",
    "        \"schema_nam\": \"Rapport_sur les coûts de dist. des addendas\",\n",
    "        \"table_nam\": \"Rapport\"\n",
    "    }\n",
    "    time_start = dt.datetime.now()\n",
    "    rv_except = fn_add_metadata_column(sdf, dict_parameter)\n",
    "    print(f\"# Execution (except) - End (duration: {(dt.datetime.now() - time_start)}) #\\n\")\n",
    "    print(f\"Return value:               {rv_except}\")\n",
    "    print(f\"Return value (status) type: {type(rv_except[0])}\")\n",
    "    print(f\"Return value (status):      {rv_except[0]}\")\n",
    "    print(f\"Return value (ex) type:     {type(rv_except[1])}\")\n",
    "    print(f\"Return value (ex):          {rv_except[1]}\")\n",
    "    print(f\"\\n----- Test fn_add_metadata_column() - End -----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a6436-31e1-4b6c-81ba-83256563f65f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c761ff02-d483-4cc8-9b3d-9a928d6ead22",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Execute \"Silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2797c3-efa0-4c18-9ab3-e0e337edce66",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def fn_execute_silver():\n",
    "    print(\"Execute Silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea9ed5f-3ad1-4b0a-8d63-c4fc82716300",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#mssparkutils.notebook.exit(\"OK\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
