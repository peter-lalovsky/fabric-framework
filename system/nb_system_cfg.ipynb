{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c35d14-abf6-494e-8c02-df71eaa656c7",
   "metadata": {
    "editable": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"defaultLakehouse\": {\"name\": \"lh_cfg\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff6f99-add8-4200-9b98-a2dfea9ccdc0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n---------- Current notenook name: nb_system_cfg ----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d2001-7bba-44a6-8a14-513d883ee885",
   "metadata": {
    "editable": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "# kill me this and above cells\n",
    "\n",
    "# Temp cell for emulating the parameters, passed by nb_master\n",
    "is_debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd464c10-7c1c-4009-ac6e-778a83e3308d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary to collect and return errors to the calling notebook\n",
    "cfg_error = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c98a8-eb73-43a2-b122-d5197c1ff7b8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if is_debug: from pyspark.sql import SparkSession as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b083e-8a76-42a3-8f40-2546a552a274",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Print current session info\n",
    "if is_debug: display(ss.getActiveSession())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3d004-c4ea-457a-af28-fd155e9642eb",
   "metadata": {
    "editable": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Get ABFS Paths of the lakehouses in the current Workspace\n",
    "try:\n",
    "    ws_id = spark.conf.get(\"trident.workspace.id\")\n",
    "    lst = mssparkutils.lakehouse.list(workspaceId = ws_id)\n",
    "\n",
    "    abfs_path_lh_cfg    = None\n",
    "    abfs_path_lh_log    = None\n",
    "    abfs_path_lh_bronze = None\n",
    "    abfs_path_lh_silver = None\n",
    "\n",
    "    for v in lst:\n",
    "        if v.displayName == \"lh_cfg\":    abfs_path_lh_cfg    = v.properties[\"abfsPath\"]\n",
    "        if v.displayName == \"lh_log\":    abfs_path_lh_log    = v.properties[\"abfsPath\"]\n",
    "        if v.displayName == \"lh_bronze\": abfs_path_lh_bronze = v.properties[\"abfsPath\"]\n",
    "        if v.displayName == \"lh_silver\": abfs_path_lh_silver = v.properties[\"abfsPath\"]\n",
    "\n",
    "    if is_debug:\n",
    "        print(\"----- ABFS Paths - Start -----\")\n",
    "        print(f\"abfs_path_lh_cfg:    {abfs_path_lh_cfg}\")\n",
    "        print(f\"abfs_path_llh_log:   {abfs_path_lh_log}\")\n",
    "        print(f\"abfs_path_lh_bronze: {abfs_path_lh_bronze}\")\n",
    "        print(f\"abfs_path_lh_silver: {abfs_path_lh_silver}\")\n",
    "        print(\"----- ABFS Paths - End -----\")\n",
    "except Exception as ex:\n",
    "    cfg_error[\"Get ABFS Paths\"] = str(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83bf29-0554-4973-a74e-ab5b9baf39eb",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Get values from table \"lh_cfg.global_parameter\"\n",
    "try:\n",
    "    sql_code = (f\"\"\"\n",
    "SELECT DISTINCT `name`, `value`\n",
    "FROM delta.`{abfs_path_lh_cfg}/Tables/global_parameter`\n",
    "    \"\"\")\n",
    "\n",
    "    sdf = spark.sql(sql_code)\n",
    "\n",
    "    if is_debug:\n",
    "        print(f\"sql_code: {sql_code}\")\n",
    "        display(sdf)\n",
    "\n",
    "    #abfs_path_lh_cfg                    = sdf.filter(sdf.name == \"abfs_path_lh_cfg\").select(sdf.value).collect()[0][0]\n",
    "    #abfs_path_lh_log                    = sdf.filter(sdf.name == \"abfs_path_lh_log\").select(sdf.value).collect()[0][0]\n",
    "    #abfs_path_lh_bronze                 = sdf.filter(sdf.name == \"abfs_path_lh_bronze\").select(sdf.value).collect()[0][0]\n",
    "    #abfs_path_lh_silver                 = sdf.filter(sdf.name == \"abfs_path_lh_silver\").select(sdf.value).collect()[0][0]\n",
    "    email_on_error = sdf.filter(sdf.name                      == \"email_on_error\").select(sdf.value).collect()[0][0]\n",
    "    days_to_keep_log = sdf.filter(sdf.name                    == \"days_to_keep_log\").select(sdf.value).collect()[0][0]\n",
    "    time_zone_nb = sdf.filter(sdf.name                        == \"time_zone_nb\").select(sdf.value).collect()[0][0]\n",
    "    time_zone_pl = sdf.filter(sdf.name                        == \"time_zone_pl\").select(sdf.value).collect()[0][0]\n",
    "    mssql_isolation_level = sdf.filter(sdf.name               == \"mssql_isolation_level\").select(sdf.value).collect()[0][0]\n",
    "    projet_documentation_data_file_name = sdf.filter(sdf.name == \"projet_documentation_data_file_name\").select(sdf.value).collect()[0][0]\n",
    "\n",
    "    if is_debug:\n",
    "        print(\"----- lh_cfg.global parameter Start -----\")\n",
    "        #print(f\"abfs_path_lh_cfg:                    {abfs_path_lh_cfg}\")\n",
    "        #print(f\"abfs_path_lh_log:                    {abfs_path_lh_log}\")\n",
    "        #print(f\"abfs_path_lh_bronze:                 {abfs_path_lh_bronze}\")\n",
    "        #print(f\"abfs_path_lh_silver:                 {abfs_path_lh_silver}\")\n",
    "        print(f\"email_on_error:                      {email_on_error}\")\n",
    "        print(f\"days_to_keep_log:                    {days_to_keep_log}\")\n",
    "        print(f\"time_zone_nb:                        {time_zone_nb}\")\n",
    "        print(f\"time_zone_pl:                        {time_zone_pl}\")\n",
    "        print(f\"mssql_isolation_level:               {mssql_isolation_level}\")\n",
    "        print(\n",
    "            f\"projet_documentation_data_file_name: {projet_documentation_data_file_name}\"\n",
    "        )\n",
    "        print(\"----- lh_cfg.global parameter End -----\")\n",
    "except Exception as ex:\n",
    "    cfg_error[\"Get global_parameter values\"] = str(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d582c-17d1-4f8f-9179-a3eef4611a75",
   "metadata": {
    "editable": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Print the config od the current session\n",
    "\n",
    "import  os\n",
    "from pyspark import SparkConf\n",
    "\n",
    "spark_config = SparkConf().getAll()\n",
    "for conf in spark_config:\n",
    "    print(conf)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
