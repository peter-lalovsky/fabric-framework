{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edfc3ac0-c951-4bae-a20f-09357e91f276",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3263fd4-0f4a-450c-9bc6-d76cdce2980c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "is_debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d934976-0931-4515-9d4f-7d016a323682",
   "metadata": {
    "editable": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "if is_debug: print(\"\\n---------- This notenook name: nb_bronze_function_extract_csv - Start ----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6e7d6-ee9e-4b6c-91be-dd318761e624",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "medallion_name = \"Bronze\"\n",
    "if is_debug: print(f\"medallion_name: {medallion_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ffb93d-6cc7-40de-a7f6-5e955c9ef2f0",
   "metadata": {
    "editable": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Current session info\n",
    "if is_debug: display(ss.getActiveSession())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ae9a7-89c9-4c59-bbd1-b56d1825890f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f38ef76-af49-4c2b-8a4a-a37d89cd4226",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Read API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e580581-adb2-481f-a99f-7b38eade7f4e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Get the list of all existing playlists again\n",
    "response = requests.get('https://cat-fact.herokuapp.com/facts/random?animal_type=cat&amount=20')\n",
    "result = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f406495-aa82-4456-9e2c-3041bc8f4ba7",
   "metadata": {
    "editable": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "# Request\n",
    "# \tMethod\n",
    "# \t\tGET\n",
    "# \t\t\tURL/key=asdasdasdasdasdasd\t\t\t(Key-Vault key)\n",
    "# \t\t\tHeader\n",
    "# \t\t\t\tBearer\n",
    "# \t\t\t\t\tUsername\n",
    "# \t\t\t\t\tPassword\n",
    "# \t\t\t\tToken\n",
    "# \t\t\tURL/?parameter1=value1&parameter2=[\"1900-01-01\", '2024-08-06']\n",
    "# \t\tPOST\n",
    "# \t\t\tURL/key=asdasdasdasdasdasd\t\t\t(Key-Vault key)\n",
    "# \t\t\tBudy --> {\"parameter1\": \"value1\", \"parameter2\": [\"1900-01-01\", '2024-08-06']}\n",
    "# \t\t\tHeader\n",
    "# \t\t\t\tBearer\n",
    "# \t\t\t\t\tUsername\n",
    "# \t\t\t\t\tPassword\n",
    "# \t\t\t\tToken\n",
    "# \tMethods\n",
    "# \t\tGET    - SELECT\n",
    "# \t\tPOST   - SELECT, UPDATE\n",
    "# \t\tDELETE - DELETE\n",
    "\n",
    "# Return\n",
    "# \tJSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e361c-e70a-473b-94c1-fbb4253cf7c3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Get api Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e4a8d-cf62-4232-8bc7-3ab7b9611fc4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "## select fromt tha tabel the parameters url\n",
    "headers = {'accept': 'content-type'}\n",
    "api_base_url = 'https://cat-fact.herokuapp.com'\n",
    "endpoint = '/facts/random'\n",
    "parameters='?animal_type=cat&amount=5'\n",
    "header= true\n",
    "\n",
    "##If header = yes\n",
    "if header is true:\n",
    "   get_methode = api_base_url + endpoint + parameters,headers=headers\n",
    "   \n",
    "else:\n",
    "   get_methode = api_base_url + endpoint + parameters\n",
    "\n",
    "#response = requests.get(api_base_url + endpoint + parameters)\n",
    "response = requests.get(api_base_url + endpoint + parameters,headers=headers)\n",
    "#response = response.headers['accept']\n",
    "#check response\n",
    "#response.content???\n",
    "\n",
    "## use response.elapsed to determine an tinmeout???\n",
    "## use response.close() at end\n",
    "## \n",
    "if response.status_code == 200:\n",
    "    result = response.text\n",
    "else:    \n",
    "    result = response.status_code\n",
    "\n",
    "\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44928246-93f5-40c3-92e1-34c9541a0df8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if \"API\" in list_technology:\n",
    "    def fn_get_api_in_lh_bronze(\n",
    "        api_base_url\n",
    "        , Header \n",
    "        , parameters_list\n",
    "        , Endpoint\n",
    "        , username\n",
    "        , password\n",
    "    ):\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        if is_debug: par = {}\n",
    "\n",
    "        try:\n",
    "            # Create empty dataframe to keep the existing files in the lakehouse\n",
    "            # if \n",
    "            # Collect \"API\", \"parameters\"\n",
    "            api_base_url = f\"\"\"SELECT DISTINCT\n",
    "    IFNULL(`folder_name`, \\\"\\\") AS `folder_name`\n",
    "    , `file_name`\n",
    "FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_excel`\n",
    "WHERE\n",
    "    `technology`       = \\\"Excel\\\"\n",
    "    AND `frequency`    IN ({str(list_frequency).replace(\"[\", \"\").replace(\"]\", \"\")})\n",
    "    AND `is_extracted` = 1;\"\"\"\n",
    "            sdf_folder_file = fn_execute_spark_sql(sql_code)[2]            \n",
    "\n",
    "            for row in sdf_folder_file.collect():\n",
    "                folder_name_clean = row.folder_name.replace(\"/\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\") # keep \"/\" while cleaning\n",
    "                folder_name_clean = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(folder_name_clean)[2], \"_\")[2]\n",
    "                folder_name_clean = folder_name_clean.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"/\")\n",
    "                file_name_clean   = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(row.file_name)[2], \"_\")[2]\n",
    "                sdf_file          = fn_list_folder(f\"{global_parameter.abfs_path_lh_bronze}/Files/incoming/{excel_folder_name}/{folder_name_clean}/{file_name_clean}.xlsx\")\n",
    "\n",
    "                for r in sdf_file[2]:\n",
    "                    new_row = spark.createDataFrame([(row.folder_name, row.file_name, folder_name_clean, r.name)], schema_exist)\n",
    "                    sdf_exist = sdf_exist.union(new_row)\n",
    "\n",
    "                sdf_exist = sdf_exist.withColumn(\"file_name\", sf.regexp_replace(\"file_name\", \".xlsx\", \"\"))\n",
    "            sdf_exist_count = sdf_exist.count()\n",
    "\n",
    "            alert             = \"Success\"\n",
    "            alert_description = f\"Existing excel files (count): {sdf_exist_count}\"\n",
    "\n",
    "            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n",
    "\n",
    "            return (alert, alert_description, sdf_exist, sdf_exist_count)\n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            return (alert, alert_description, None, None)\n",
    "        finally:\n",
    "            if is_debug:\n",
    "                par[\"locals\"] = locals()\n",
    "                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n",
    "                fn_print_debug_info(alert, fn_name, par)\n",
    "                del par\n",
    "\n",
    "        fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d169600-a708-4269-a1ab-2df061968d51",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14477d3-c013-4743-8be5-22ff3fc678d2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Extract CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5400f03-d217-40ac-b585-04245c669a61",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if \"CSV\" in list_technology:\n",
    "    def fn_extract_csv(\n",
    "        technology\n",
    "        , frequency\n",
    "    ):\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        if is_debug: par = {}\n",
    "\n",
    "        try:\n",
    "            # Rename the existing files in lh_bronze\n",
    "            sdf_renamed = fn_rename_files_in_lh_bronze(technology, frequency, \"Archive\")[1]\n",
    "\n",
    "            # Get extract table list\n",
    "            dict_parameter_table_list     = {\"technology\": technology, \"frequency\": frequency, \"action\": \"Extract\"}\n",
    "            rv_extract_table_list         = fn_get_extract_table_list(dict_parameter_table_list)\n",
    "            sdf_extract_table_list_status = rv_extract_table_list[0]\n",
    "            sdf_extract_table_list        = rv_extract_table_list[2]\n",
    "            sdf_extract_table_list_count  = rv_extract_table_list[3]\n",
    "\n",
    "            if is_debug:\n",
    "                print(f\"sdf_extract_table_list_status: {sdf_extract_table_list_status}\")\n",
    "                print(f\"sdf_extract_table_list:\")\n",
    "                display(sdf_extract_table_list)\n",
    "                print(f\"sdf_extract_table_list_count:  {sdf_extract_table_list_count}\")\n",
    "\n",
    "            # Loop extract table list\n",
    "            if sdf_extract_table_list_count > 0:\n",
    "                # etl_r --> sdf_extract_table_list.row\n",
    "                for etl_r in sdf_extract_table_list \\\n",
    "                    .sort(sdf_extract_table_list.technology.asc(),\n",
    "                        sdf_extract_table_list.frequency.asc(),                        \n",
    "                        sdf_extract_table_list.folder_name.asc(),\n",
    "                        sdf_extract_table_list.file_name.asc()\n",
    "                    ).collect():\n",
    "\n",
    "                    dict_parameter = {\"process_timestamp\": global_parameter.process_timestamp\n",
    ", \"technology\": etl_r.technology\n",
    ", \"frequency\": etl_r.frequency\n",
    ", \"folder_name\": etl_r.folder_name\n",
    ", \"file_name\": etl_r.file_name\n",
    ", \"has_header\": etl_r.has_header\n",
    ", \"delimiter\": etl_r.delimiter\n",
    ", \"row_separator\": etl_r.row_separator\n",
    ", \"quote\": etl_r.quote}\n",
    "\n",
    "                    if is_debug:\n",
    "                        print(f\"sdf_extract_table_list.etl_r:\")\n",
    "                        print(f\"  technology:    {etl_r.technology}\")\n",
    "                        print(f\"  frequency:     {etl_r.frequency}\")\n",
    "                        print(f\"  folder_name:   {etl_r.folder_name}\")\n",
    "                        print(f\"  file_name:     {etl_r.file_name}\")                    \n",
    "                        print(f\"  has_header:    {etl_r.has_header}\")\n",
    "                        print(f\"  delimiter:     {etl_r.delimiter}\")\n",
    "                        print(f\"  row_separator: {etl_r.row_separator}\")\n",
    "                        print(f\"  quote:         {etl_r.quote}\")\n",
    "                        print(f\"dict_parameter: {dict_parameter}\")\n",
    "                    \n",
    "                    # Clean up \"technology\", \"folder_name\", \"file_name\" - Start\n",
    "                    # Clean accents and not alphanumerics\n",
    "                    technology_clean  = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(technology)[2], \"_\")[2]\n",
    "                    folder_name_clean = etl_r.folder_name.replace(\"/\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\") # keep \"/\" while cleaning\n",
    "                    folder_name_clean = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(folder_name_clean)[2], \"_\")[2]\n",
    "                    folder_name_clean = folder_name_clean.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"/\")\n",
    "                    file_name_clean   = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(etl_r.file_name)[2], \"_\")[2]\n",
    "\n",
    "                    dict_parameter[\"technology_clean\"]  = technology_clean\n",
    "                    dict_parameter[\"folder_name_clean\"] = folder_name_clean\n",
    "                    dict_parameter[\"file_name_clean\"]   = file_name_clean\n",
    "\n",
    "                    if is_debug:\n",
    "                        print(f\"folder_name_clean: >{folder_name_clean}<\")\n",
    "                        print(f\"file_name_clean: >{file_name_clean}<\")\n",
    "                        print(f\"dict_parameter: {dict_parameter}\")\n",
    "                    # Clean up \"technology\", \"folder_name\", \"file_name\" - End\n",
    "\n",
    "                    # Generate source file path and check if file exists - Start\n",
    "                    csv_file_path_clean = f\"{global_parameter.abfs_path_lh_bronze}/Files/incoming/{etl_r.technology}/{folder_name_clean}/{file_name_clean}.csv\"\n",
    "                    csv_file_path_clean = csv_file_path_clean.replace(\"abfss://\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n",
    "                    csv_file_path_clean = csv_file_path_clean.replace(\"//\", \"/\")\n",
    "                    csv_file_path_clean = csv_file_path_clean.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"abfss://\")\n",
    "\n",
    "                    does_csv_file_exist = fn_does_file_exist(csv_file_path_clean, dict_parameter)[2]\n",
    "\n",
    "                    dict_parameter[\"csv_file_path_clean\"] = csv_file_path_clean\n",
    "                    dict_parameter[\"does_csv_file_exist\"] = does_csv_file_exist\n",
    "\n",
    "                    if is_debug:\n",
    "                        print(f\"csv_file_path_clean: {csv_file_path_clean}\")\n",
    "                        print(f\"does_csv_file_exist: {does_csv_file_exist}\")\n",
    "                        print(f\"dict_parameter: {dict_parameter}\")\n",
    "                    # Generate source file path and check if file exists - End\n",
    "                    \n",
    "                    if does_csv_file_exist == True:\n",
    "                        # Extract file - Start\n",
    "                        rv_csv_file         = fn_read_csv_file(dict_parameter)\n",
    "                        sdf_csv_file_status = rv_csv_file[0]\n",
    "                        sdf_csv_file        = rv_csv_file[2]\n",
    "                        sdf_csv_file_count  = rv_csv_file[3]\n",
    "\n",
    "                        if is_debug:\n",
    "                            print(f\"rv_csv_file:         {rv_csv_file}\")\n",
    "                            print(f\"sdf_csv_file_status: {sdf_csv_file_status}\")\n",
    "                            print(f\"sdf_csv_file (schema):\")\n",
    "                            sdf_csv_file.printSchema()\n",
    "                            print(f\"sdf_csv_file:\")\n",
    "                            display(sdf_csv_file)\n",
    "                            print(f\"sdf_csv_file_count:  {sdf_csv_file_count}\")\n",
    "                        # Extract file - End\n",
    "\n",
    "                        # Save CSV file as table in \"lh_bronze\" - Start\n",
    "                        # Replace \"/\" with \"_\" in \"folder_name\"\n",
    "                        folder_name_in_table_name = folder_name_clean.replace(\"/\", \"_\")\n",
    "\n",
    "                        # Generate full table path\n",
    "                        csv_file_table_path = f\"{global_parameter.abfs_path_lh_bronze}/Tables/{technology_clean}_{folder_name_in_table_name}_{file_name_clean}\"\n",
    "\n",
    "                        # Remove double underscore (folder_name is empty)\n",
    "                        csv_file_table_path = csv_file_table_path.replace(f\"__\", \"_\")\n",
    "\n",
    "                        if is_debug: print(f\"sdf_csv_file_table_path: {csv_file_table_path}\")\n",
    "\n",
    "                        fn_save_sdf_as_table(\n",
    "                            sdf_csv_file\n",
    "                            , \"delta\"\n",
    "                            , \"overwrite\"\n",
    "                            , csv_file_table_path\n",
    "                            , dict_parameter\n",
    "                        )\n",
    "                        # Save CSV file as table in \"lh_bronze\" - End\n",
    "\n",
    "                        # Copy and delete CSV file to \"lh_bronze/Processed/CSV\" (archive) - Start\n",
    "                        now_year  = fn_get_now(\"year\")[1]\n",
    "                        now_month = fn_get_now(\"month\")[1]\n",
    "                        dstn_file_path = f\"{global_parameter.abfs_path_lh_bronze}/Files/processed/{technology_clean}/\\\n",
    "{folder_name_clean}/{file_name_clean}/\\\n",
    "{now_year}/{now_month}/\\\n",
    "{file_name_clean}_{global_parameter.process_timestamp}.csv\"\n",
    "\n",
    "                        dstn_file_path = dstn_file_path.replace(\"abfss://\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n",
    "                        dstn_file_path = dstn_file_path.replace(\"//\", \"/\")\n",
    "                        dstn_file_path = dstn_file_path.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"abfss://\")\n",
    "\n",
    "                        copy_file_status = fn_copy_file(csv_file_path_clean, dstn_file_path, dict_parameter)[0]\n",
    "                        if copy_file_status == \"Success\": fn_delete_file(csv_file_path_clean, dict_parameter)\n",
    "\n",
    "                        if is_debug:\n",
    "                            print(f\"csv_file_path_clean: {csv_file_path_clean}\")\n",
    "                            print(f\"dstn_file_path: {dstn_file_path}\")\n",
    "                        # Copy and delete CSV file to \"lh_bronze/Processed/CSV\" (archive) - End\n",
    "\n",
    "                        # Insert \"Success\" in lh_log (for each file) - Start\n",
    "                        # Always log \"Success\" for extracted object.\n",
    "                        alert             = \"Success\"\n",
    "                        alert_description = f\"Row count: {str(sdf_csv_file_count)}\"\n",
    "\n",
    "                        if is_debug:\n",
    "                            par[\"locals\"] = locals()\n",
    "                            fn_print_debug_info(alert, fn_name, par)\n",
    "                            del par\n",
    "\n",
    "                        fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)\n",
    "                        # Insert \"Success\" in lh_log (for each file) - End\n",
    "                        \n",
    "                        if is_debug: print(\"--------------- Loop - End ---------------\")\n",
    "\n",
    "            return (alert, alert_description)\n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            if is_debug:\n",
    "                par[\"locals\"] = locals()\n",
    "                fn_print_debug_info(alert, fn_name, par)\n",
    "                del par\n",
    "\n",
    "            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)\n",
    "\n",
    "            return (alert, alert_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb92bb-4c00-44ce-a6b4-2bc8ae8dfc0b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if is_debug: print(\"\\n---------- This notenook name: nb_bronze_function_extract_csv - End ----------\\n\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
