{"cells":[{"cell_type":"markdown","source":["# System"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3783b27e-68b2-43ef-a5f4-677b10bb345d"},{"cell_type":"code","source":["if is_debug: print(\"\\n---------- This notenook name: nb_bronze_function - Start ----------\\n\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b13cef20-aea8-4dd5-8008-24bda597dca3"},{"cell_type":"code","source":["medallion_name = \"Bronze\"\n","if is_debug: print(f\"medallion_name: {medallion_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"2234db21-912d-40e2-a018-f82c6d161621"},{"cell_type":"code","source":["# Current session info\n","if is_debug: display(ss.getActiveSession())"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"40ce5bb0-b681-4a0f-b7aa-3a0af5cfda5c"},{"cell_type":"markdown","source":["# Function"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1ceb4022-a2f7-4249-84c1-e042819f56ba"},{"cell_type":"markdown","source":["## Get existing technology list"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"19a20624-e2be-4078-ad33-7c62f171c58a"},{"cell_type":"code","source":["def fn_get_existing_technology_list(list_frequency):\n","    fn_name        = stk()[0][3]\n","    caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","    if is_debug: par = {}\n","\n","    try:\n","        sql_code = f\"\"\"\n","SELECT DISTINCT       `technology` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_csv`        WHERE `is_extracted` = 1 AND `frequency` IN ({str(list_frequency).replace(\"[\", \"\").replace(\"]\", \"\")})\n","UNION SELECT DISTINCT `technology` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_excel`      WHERE `is_extracted` = 1 AND `frequency` IN ({str(list_frequency).replace(\"[\", \"\").replace(\"]\", \"\")})\n","UNION SELECT DISTINCT `technology` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_json`       WHERE `is_extracted` = 1 AND `frequency` IN ({str(list_frequency).replace(\"[\", \"\").replace(\"]\", \"\")})\n","UNION SELECT DISTINCT `technology` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_lakehouse`  WHERE `is_extracted` = 1 AND `frequency` IN ({str(list_frequency).replace(\"[\", \"\").replace(\"]\", \"\")})\n","UNION SELECT DISTINCT `technology` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_sql_server` WHERE `is_extracted` = 1 AND `frequency` IN ({str(list_frequency).replace(\"[\", \"\").replace(\"]\", \"\")})\n","UNION SELECT DISTINCT `technology` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_api`        WHERE `is_extracted` = 1 AND `frequency` IN ({str(list_frequency).replace(\"[\", \"\").replace(\"]\", \"\")});\n","        \"\"\"\n","        rv              = fn_execute_spark_sql(sql_code)\n","        sdf             = rv[2]\n","        sdf_count       = rv[3]\n","        list_technology = [row.technology for row in sdf.collect()]\n","\n","        # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","        alert             = \"Success\"\n","        alert_description = f\"Technology list: {list_technology}\"\n","\n","        return (alert, alert_description, list_technology, sdf_count)\n","    except Exception as ex:\n","        alert             = \"Danger\"\n","        alert_description = str(ex)\n","\n","        return (alert, alert_description, None, None)\n","    finally:\n","        if is_debug:\n","            par[\"locals\"] = locals()\n","            fn_print_debug_info(alert, fn_name, par)\n","            del par\n","\n","        fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}},"editable":false,"run_control":{"frozen":true}},"id":"467f28aa-c0fb-4233-8366-7cab9e8c3311"},{"cell_type":"code","source":["# Get Existing Technology List\n","list_technology = fn_get_existing_technology_list(list_frequency)[2]\n","if is_debug: print(f\"list_technology: {list_technology}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}},"editable":false,"run_control":{"frozen":true}},"id":"43cff85e-90bf-4b31-b480-2d3cdfb7cfe6"},{"cell_type":"markdown","source":["## Get a list of the extracted tables"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"864e596d-b449-414d-a9fb-c94800f3b00f"},{"cell_type":"code","source":["def fn_get_extract_table_list(dict_parameter):\n","    fn_name        = stk()[0][3]\n","    caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","    if is_debug: par = {}\n","\n","    try:\n","        if \\\n","            dict_parameter[\"technology\"] == \"Excel\" \\\n","            and dict_parameter[\"action\"] == \"Extract\":\n","\n","            sql_query = f\"\"\"SELECT DISTINCT\n","    `technology`\n","    , `frequency`\n","    , IFNULL(`folder_name`   , \\\"\\\") AS `folder_name`\n","    , IFNULL(`file_name`     , \\\"\\\") AS `file_name`\n","    , IFNULL(`worksheet_name`, \\\"\\\") AS `worksheet_name`\n","    , `first_row`\n","FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_excel`\n","WHERE\n","    `frequency`        = \\\"{dict_parameter[\"frequency\"]}\\\"\n","    AND `is_extracted` = 1\n","ORDER BY\n","    `technology`\n","    , `frequency`\n","    , IFNULL(`folder_name`   , \\\"\\\")\n","    , IFNULL(`file_name`     , \\\"\\\")\n","    , IFNULL(`worksheet_name`, \\\"\\\");\"\"\"\n","\n","        elif \\\n","            dict_parameter[\"technology\"] == \"Excel\" \\\n","            and dict_parameter[\"action\"] == \"Archive\":\n","\n","            sql_query = f\"\"\"SELECT DISTINCT\n","    `technology`\n","    , `frequency`\n","    , IFNULL(`folder_name`, \\\"\\\") AS `folder_name`\n","    , IFNULL(`file_name`  , \\\"\\\") AS `file_name`\n","FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_excel`\n","WHERE\n","    `frequency`        = \\\"{dict_parameter[\"frequency\"]}\\\"\n","    AND `is_extracted` = 1\n","ORDER BY\n","    `frequency`\n","    , IFNULL(`folder_name`, \\\"\\\")\n","    , IFNULL(`file_name`  , \\\"\\\");\"\"\"\n","\n","        elif \\\n","            (dict_parameter[\"technology\"] == \"SQL Server\" or dict_parameter[\"technology\"] == \"Lakehouse\") \\\n","            and dict_parameter[\"action\"]  == \"Extract\":\n","\n","            sql_query = (f\"\"\"SELECT DISTINCT\n","    A.`technology`\n","    , A.`frequency`\n","    , IFNULL(A.`lakehouse_name`               , \\\"\\\") AS `lakehouse_name`\n","    , IFNULL(A.`server_name`                  , \\\"\\\") AS `server_name`\n","    , IFNULL(A.`database_name`                , \\\"\\\") AS `database_name`\n","    , IFNULL(A.`schema_name`                  , \\\"\\\") AS `schema_name`\n","    , IFNULL(A.`table_name`                   , \\\"\\\") AS `table_name`\n","    , IFNULL(A.`keyvault_secret_name_user`    , \\\"\\\") AS `keyvault_secret_name_user`\n","    , IFNULL(A.`keyvault_url`                 , \\\"\\\") AS `keyvault_url`\n","    , IFNULL(A.`keyvault_secret_name_password`, \\\"\\\") AS `keyvault_secret_name_password`\n","FROM\n","    (\n","        SELECT `technology`, `frequency`, \\\"\\\" AS `lakehouse_name`, `server_name`, `database_name`, `schema_name`, `table_name`, `keyvault_secret_name_user`, `keyvault_url`, `keyvault_secret_name_password`, `is_extracted` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_sql_server`\n","        UNION SELECT `technology`, `frequency`, `lakehouse_name`, \\\"\\\" AS `server_name`, \\\"\\\" AS `database_name`, \\\"\\\" AS `schema_name`, `table_name`, \\\"\\\" AS `keyvault_secret_name_user`, \\\"\\\" AS `keyvault_url`, \\\"\\\" AS `keyvault_secret_name_password`, `is_extracted` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_lakehouse`\n","    ) AS A\n","WHERE\n","    A.`technology`      = \\\"{dict_parameter[\"technology\"]}\\\"\n","    AND A.`frequency`   = \\\"{dict_parameter[\"frequency\"]}\\\"\n","    AND A.`is_extracted`= 1\n","ORDER BY\n","    `technology`\n","    , `frequency`\n","    , IFNULL(`lakehouse_name`  , \\\"\\\")\n","    , IFNULL(`server_name`  , \\\"\\\")\n","    , IFNULL(`database_name`, \\\"\\\")\n","    , IFNULL(`schema_name`  , \\\"\\\")\n","    , IFNULL(`table_name`   , \\\"\\\");\"\"\")\n","\n","#         elif \\\n","#             dict_parameter[\"technology\"] == \"Lakehouse\" \\\n","#             and dict_parameter[\"action\"] == \"Extract\":\n","\n","#             sql_query = (f\"\"\"SELECT DISTINCT\n","#     `technology`\n","#     , `frequency`\n","#     , IFNULL(`lakehouse_name`, \\\"\\\") AS `lakehouse_name`\n","#     , IFNULL(`table_name`    , \\\"\\\") AS `table_name`\n","# FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_lakehouse`\n","# WHERE\n","#     `frequency`       = \\\"{dict_parameter[\"frequency\"]}\\\"\n","#     AND `is_extracted`= 1\n","# ORDER BY\n","#     `technology`\n","#     , `frequency`\n","#     , IFNULL(`lakehouse_name`, \\\"\\\")\n","#     , IFNULL(`table_name`    , \\\"\\\");\"\"\")\n","\n","        elif \\\n","            dict_parameter[\"technology\"] == \"CSV\" \\\n","            and (dict_parameter[\"action\"] == \"Extract\" or dict_parameter[\"action\"] == \"Archive\"):\n","\n","            sql_query = (f\"\"\"SELECT DISTINCT\n","    `technology`\n","    , `frequency`\n","    , IFNULL(`folder_name`       , \\\"\\\") AS `folder_name`\n","    , IFNULL(`file_name`         , \\\"\\\") AS `file_name`\n","    , IFNULL(STRING(`has_header`), \\\"\\\") AS `has_header`\n","    , IFNULL(`delimiter`         , \\\"\\\") AS `delimiter`\n","    , IFNULL(`row_separator`     , \\\"\\\") AS `row_separator`\n","    , IFNULL(`quote`             , \\\"\\\") AS `quote`\n","FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_csv`\n","WHERE\n","    `frequency`        = \\\"{dict_parameter[\"frequency\"]}\\\"\n","    AND `is_extracted` = 1\n","ORDER BY\n","    `technology`\n","    , `frequency`\n","    , IFNULL(`folder_name`, \\\"\\\")\n","    , IFNULL(`file_name` , \\\"\\\");\"\"\")\n","\n","        elif \\\n","            dict_parameter[\"technology\"] == \"JSON\" \\\n","            and (dict_parameter[\"action\"] == \"Extract\" or dict_parameter[\"action\"] == \"Archive\"):\n","\n","            sql_query = (f\"\"\"SELECT DISTINCT\n","    `technology`\n","    , `frequency`\n","    , IFNULL(`folder_name`, \\\"\\\") AS `folder_name`\n","    , IFNULL(`file_name`  , \\\"\\\") AS `file_name`\n","FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_json`\n","WHERE\n","    `frequency`        = \\\"{dict_parameter[\"frequency\"]}\\\"\n","    AND `is_extracted` = 1\n","ORDER BY\n","    `technology`\n","    , `frequency`\n","    , IFNULL(`folder_name`, \\\"\\\")\n","    , IFNULL(`file_name`  , \\\"\\\");\"\"\")\n","\n","        sdf = fn_execute_spark_sql(sql_query)[2]\n","        sdf_count = sdf.count()\n","\n","        alert             = \"Success\"\n","        alert_description = f\"Table/file list (count): {sdf_count}\"\n","\n","        # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","        return (alert, alert_description, sdf, sdf_count)\n","    except Exception as ex:\n","        alert             = \"Danger\"\n","        alert_description = str(ex)        \n","\n","        return (alert, alert_description, None, None)\n","    finally:\n","        if is_debug:\n","            par[\"locals\"] = locals()\n","            fn_print_debug_info(alert, fn_name, par)\n","            del par\n","\n","        fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"d3cba043-a928-4eea-96a9-4f850bb7213d"},{"cell_type":"markdown","source":["## Get a list of the columns of the extracted tables"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"6f23f3e6-6f9b-478a-9f58-7b8439af78be"},{"cell_type":"code","source":["def fn_get_extract_column(dict_parameter):\n","    fn_name        = stk()[0][3]\n","    caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","    if is_debug: par = {}\n","\n","    try:\n","        if dict_parameter[\"technology\"] == \"SQL Server\" or dict_parameter[\"technology\"] == \"Lakehouse\":\n","            if dict_parameter[\"technology\"] == \"SQL Server\": lakehouse_name = \"\"\n","            else:                                            lakehouse_name = dict_parameter[\"lakehouse_name\"]\n","            \n","            if dict_parameter[\"technology\"] == \"SQL Server\": server_alias = dict_parameter[\"server_alias\"]\n","            else:                                            server_alias = \"\"\n","\n","            if dict_parameter[\"technology\"] == \"SQL Server\": database_name = dict_parameter[\"database_name\"]\n","            else:                                            database_name = \"\"\n","\n","            if dict_parameter[\"technology\"] == \"SQL Server\": schema_name = dict_parameter[\"schema_name\"]\n","            else:                                            schema_name = \"\"\n","            \n","            table_name = dict_parameter[\"table_name\"]\n","\n","            sql_query = f\"\"\"SELECT DISTINCT\n","    `sequence`\n","    , IFNULL(A.`prefix_select`, \\\"\\\")  AS `prefix_select`\n","    , IFNULL(A.`column_name`  , \\\"\\\")  AS `column_name`\n","    , TRIM(IFNULL(CONCAT(\n","        CASE WHEN A.`sequence` = 1 THEN '' ELSE ', ' END,\n","        CONCAT(IF(A.`prefix_select` IS NOT NULL, CONCAT(A.`prefix_select`, '.'), ''), '[', A.`column_name`, '] AS [', A.`column_name`, ']')\n","    )                         , \\\"\\\")) AS `alias`\n","    , IFNULL(A.`from`         , \\\"\\\")  AS `from`\n","    , IFNULL(A.`where`        , \\\"\\\")  AS `where`\n","    , A.`is_pk`                        AS `is_pk`\n","    , IFNULL(A.`data_type`    , \\\"\\\")  AS `data_type`\n","FROM\n","  (\n","          SELECT `technology`, `frequency`, `server_name`, `database_name`, `schema_name`, `table_name`, \\\"\\\" AS `lakehouse_name`, `sequence`, `prefix_select`, `column_name`, `from`, `where`, `is_pk`, `data_type`, `is_extracted` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_sql_server`\n","    UNION SELECT `technology`, `frequency`, \\\"\\\" AS `server_name`, \\\"\\\" AS `database_name`, \\\"\\\" AS `schema_name`, `table_name`, `lakehouse_name`, `sequence`, `prefix_select`, `column_name`, `from`, `where`, `is_pk`, `data_type`, `is_extracted` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_lakehouse`\n","  ) AS A\n","WHERE\n","    A.`technology`                       = \\\"{dict_parameter[\"technology\"]}\\\"\n","    AND A.`frequency`                    = \\\"{dict_parameter[\"frequency\"]}\\\"\n","    AND IFNULL(A.`lakehouse_name`, \\\"\\\") = \\\"{lakehouse_name}\\\"\n","    AND IFNULL(A.`server_name`   , \\\"\\\") = \\\"{server_alias}\\\"\n","    AND IFNULL(A.`database_name` , \\\"\\\") = \\\"{database_name}\\\"\n","    AND IFNULL(A.`schema_name`   , \\\"\\\") = \\\"{schema_name}\\\"\n","    AND IFNULL(A.`table_name`    , \\\"\\\") = \\\"{table_name}\\\"\n","    AND A.`is_extracted`                 = 1\n","ORDER BY CAST(`sequence` AS BIGINT);\"\"\"\n","\n","        elif dict_parameter[\"technology\"] == \"Excel\":\n","            sql_query = f\"\"\"SELECT\n","    `sequence`\n","    , IFNULL(`column_name`, \\\"\\\") AS `column_name`\n","    , `is_pk`\n","    , IFNULL(`data_type`  , \\\"\\\") AS `data_type`\n","FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_excel`\n","WHERE\n","    `technology`                       = \\\"{dict_parameter[\"technology\"]}\\\"\n","    AND `frequency`                    = \\\"{dict_parameter[\"frequency\"]}\\\"\n","    AND IFNULL(`folder_name`   , \\\"\\\") = \\\"{dict_parameter[\"folder_name\"]}\\\"\n","    AND IFNULL(`file_name`     , \\\"\\\") = \\\"{dict_parameter[\"file_name\"]}\\\"\n","    AND IFNULL(`worksheet_name`, \\\"\\\") = \\\"{dict_parameter[\"worksheet_name\"]}\\\"\n","    AND `is_extracted`                 = 1\n","ORDER BY CAST(`sequence` AS BIGINT);\"\"\"\n","\n","        elif dict_parameter[\"technology\"] == \"CSV\":\n","            sql_query = f\"\"\"SELECT\n","    `sequence`\n","    , IFNULL(`column_name`  , \\\"\\\") AS `column_name`\n","    , `is_pk`\n","    , IFNULL(`data_type`    , \\\"\\\") AS `data_type`\n","    , `has_header`\n","    , IFNULL(`delimiter`    , \\\"\\\") AS `delimiter`\n","    , IFNULL(`row_separator`, \\\"\\\") AS `row_separator`\n","    , IFNULL(`quote`        , \\\"\\\") AS `quote`\n","FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_csv`\n","WHERE\n","    `technology`                       = \\\"{dict_parameter[\"technology\"]}\\\"\n","    AND `frequency`                    = \\\"{dict_parameter[\"frequency\"]}\\\"\n","    AND IFNULL(`folder_name`   , \\\"\\\") = \\\"{dict_parameter[\"folder_name\"]}\\\"\n","    AND IFNULL(`file_name`     , \\\"\\\") = \\\"{dict_parameter[\"file_name\"]}\\\"\n","    AND `is_extracted`                 = 1\n","ORDER BY CAST(`sequence` AS BIGINT);\"\"\"\n","\n","        elif dict_parameter[\"technology\"] == \"JSON\":\n","            sql_query = f\"\"\"SELECT\n","    `sequence`\n","    , IFNULL(`column_name`       , \\\"\\\") AS `column_name`\n","    , IFNULL(`parent_column_name`, \\\"\\\") AS `parent_column_name`\n","    , `is_pk`\n","    , IFNULL(`data_type`         , \\\"\\\") AS `data_type`\n","FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_json`\n","WHERE\n","    `technology`                       = \\\"{dict_parameter[\"technology\"]}\\\"\n","    AND `frequency`                    = \\\"{dict_parameter[\"frequency\"]}\\\"\n","    AND IFNULL(`folder_name`   , \\\"\\\") = \\\"{dict_parameter[\"folder_name\"]}\\\"\n","    AND IFNULL(`file_name`     , \\\"\\\") = \\\"{dict_parameter[\"file_name\"]}\\\"\n","    AND `is_extracted`                 = 1\n","ORDER BY CAST(`sequence` AS BIGINT);\"\"\"\n","\n","        sdf       = fn_execute_spark_sql(sql_query)[2]\n","        sdf_count = sdf.count()\n","\n","        alert             = \"Success\"\n","        alert_description = f\"Column list (count): {sdf_count}\"\n","\n","        # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","        return (alert, alert_description, sdf, sdf_count)\n","    except Exception as ex:\n","        alert             = \"Danger\"\n","        alert_description = str(ex)\n","\n","        return (alert, alert_description, None, None)\n","    finally:\n","        if is_debug:\n","            par[\"locals\"] = locals()\n","            if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","            fn_print_debug_info(alert, fn_name, par)\n","            del par\n","\n","        fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"b0452664-9273-4502-b49f-76c2688d4b9d"},{"cell_type":"markdown","source":["## Dataframe"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bb007502-dbc1-4e18-8a4c-b3041bf280a7"},{"cell_type":"markdown","source":["### Save in single .parquet file"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"a5c9c1c5-492b-4909-8e92-e42fddf75e31"},{"cell_type":"code","source":["# Used for archiving\n","def fn_save_in_single_file(\n","    sdf\n","    , file_name\n","):\n","    fn_name        = stk()[0][3]\n","    caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","    if is_debug: par = {}\n","\n","    try:\n","        # Add column \"process_timestamp\"\n","        sdf       = sdf.withColumn(\"process_timestamp\", sf.lit(global_parameter.process_timestamp))\n","        sdf_count = sdf.count()\n","\n","        # Cast and collect all Timestamp columns to String\n","        columns = []\n","        for col in sdf.dtypes:\n","            if col[1] == \"timestamp\":\n","                sdf = sdf.withColumn(col[0], sf.date_format(col[0], \"yyyy-MM-dd HH:mm:ss.SSSSSS\"))\n","                columns.append(col[0])\n","\n","        # Convert SDF to PDF\n","        pdf = sdf.toPandas()\n","\n","        # Cast the collected Timestamp columns to Pandas DateTime\n","        for col in columns: pdf[col] = pd.to_datetime(pdf[col])\n","        \n","        # Save in single .parquet file\n","        pdf.to_parquet(file_name, index = False)\n","\n","        alert             = \"Success\"\n","        alert_description = f\"Saved in single file (name): {file_name}\"\n","\n","        # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","        return (alert, alert_description)\n","    except Exception as ex:\n","        alert             = \"Danger\"\n","        alert_description = str(ex)\n","\n","        return (alert, alert_description)\n","    finally:\n","        if is_debug:\n","            par[\"locals\"] = locals()\n","            if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","            fn_print_debug_info(alert, fn_name, par)\n","            del par\n","\n","        pdf = pdf.head(5)\n","        fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"334c999c-6225-42a6-aa14-a34d2991cb2a"},{"cell_type":"markdown","source":["# Operation"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"805e1562-8980-42dc-88e7-b189e3e6c242"},{"cell_type":"markdown","source":["## Rename existing files in \"lh_bronze\""],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7d60f5c9-b147-4b21-99fd-9e370dfbb6f2"},{"cell_type":"code","source":["# Clean from filename:\n","#    Accents\n","#    All non-alphanumerical (keep only [A-Z][a-z][0-9]_)\n","\n","if (\"Excel\" in list_technology or \"CSV\" in list_technology or \"JSON\" in list_technology):\n","    def fn_rename_files_in_lh_bronze(technology, frequency, action):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            # Set file_extention\n","            if technology   == \"Excel\": file_extention = \"xlsx\"\n","            elif technology == \"CSV\":   file_extention = \"csv\"\n","            elif technology == \"JSON\":  file_extention = \"json\"\n","\n","            # Get table list\n","            dict_parameter = {\"technology\": technology, \"frequency\": frequency, \"action\": action}\n","            sdf = fn_get_extract_table_list(dict_parameter)[2]\n","            sdf_count = sdf.count()\n","\n","            # Loop archive table list\n","            for row in sdf \\\n","                .sort(\n","                    sdf.technology.asc()\n","                    , sdf.frequency.asc()\n","                    , sdf.folder_name.asc()\n","                    , sdf.file_name.asc()\n","                ) \\\n","                .collect():\n","\n","                dict_parameter = {\"technology\": row.technology, \"frequency\": row.frequency, \"folder_name\": row.folder_name, \"file_name\": row.file_name}\n","\n","                # Clean\n","                folder_name_clean = row.folder_name.replace(\"/\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\") # keep \"/\" while cleaning\n","                folder_name_clean = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(folder_name_clean)[2], \"_\")[2]\n","                folder_name_clean = folder_name_clean.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"/\")\n","                file_name_clean   = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(row.file_name)[2], \"_\")[2]\n","\n","                # Rename file\n","                dict_parameter[\"folder_name_clean\"] = folder_name_clean\n","                dict_parameter[\"file_name_clean\"]   = file_name_clean\n","                \n","                src = f\"{global_parameter.abfs_path_lh_bronze}/Files/incoming/{row.technology}/{row.folder_name}/{row.file_name}.{file_extention}\"\n","                src = src.replace(\"abfss://\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n","                src = src.replace(\"//\", \"/\")\n","                src = src.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"abfss://\")\n","                \n","                dstn = f\"{global_parameter.abfs_path_lh_bronze}/Files/incoming/{row.technology}/{folder_name_clean}/{file_name_clean}.{file_extention}\"\n","                dstn = dstn.replace(\"abfss://\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n","                dstn = dstn.replace(\"//\", \"/\")\n","                dstn = dstn.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"abfss://\")\n","                fn_move_file(src, dstn, dict_parameter)\n","\n","            alert             = \"Success\"\n","            alert_description = f\"Expected {technology} files to rename (count): {sdf_count}\"            \n","            \n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, sdf)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","        fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"44324db9-d8bd-40f3-bfd5-53fee7fbc032"},{"cell_type":"markdown","source":["## Create lookup SDF \"server_alias - server_name\""],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8fcd64ee-26c2-40d0-8872-2f5406cb581b"},{"cell_type":"code","source":["# Used to apply \"SQL Server aliases\"\n","if \"SQL Server\" in list_technology or \"Lakehouse\" in list_technology:\n","    def fn_create_lookup_server_alias_server_name():\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","        \n","        try:\n","            sql_code = f\"\"\"SELECT DISTINCT\n","    EO.`server_name` AS `server_alias`\n","    , GP.`value`     AS `server_name`\n","FROM\n","    delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_sql_server` AS EO\n","    JOIN delta.`{global_parameter.abfs_path_lh_cfg}/Tables/global_parameter`     AS GP ON EO.`server_name` = GP.`name`\n","WHERE EO.`is_extracted` = 1;\"\"\"\n","\n","            rv_lookup_server_alias_server_name = fn_execute_spark_sql(sql_code)\n","            sdf                                = rv_lookup_server_alias_server_name[2]\n","            sdf_count                          = rv_lookup_server_alias_server_name[3]\n","\n","            dict_server_alias_server_name = {}\n","            for row in sdf.collect(): dict_server_alias_server_name[row.server_alias] = row.server_name\n","\n","            alert             = \"Success\"\n","            alert_description = f\"Lookup SDF \\\"server_alias - server_name\\\" (count): {sdf_count}\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, dict_server_alias_server_name, sdf_count)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","        fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"927cc3f1-3cf7-4ad2-b3cf-e600b51faf14"},{"cell_type":"markdown","source":["## Extract Preparation"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"3645d112-57a7-4676-a1a5-53530c1f49a3"},{"cell_type":"code","source":["def fn_extract_preparation(list_technology):   \n","    fn_name        = stk()[0][3]\n","    caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","    if is_debug: par = {}\n","\n","    try:\n","        # Clean up log\n","        fn_lh_log_log_cleanup(global_parameter.days_to_keep_log)\n","\n","        # Insert SimpleNamespace.global_parameter in \"lh_log.log\"\n","        fn_local_log_insert_global_parameter()\n","\n","        # Delete the tables in \"lh_bronze\"\n","        fn_delete_all_in_folder(f\"{global_parameter.abfs_path_lh_bronze}/Tables\")\n","\n","        # Create lookup \"server_alias - server_name\"\n","        if \"SQL Server\" in list_technology: dict_server_alias_server_name = fn_create_lookup_server_alias_server_name()[2]\n","\n","        alert             = \"Success\"\n","        alert_description = \"\"\n","\n","        # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","        return (alert, alert_description, dict_server_alias_server_name)\n","    except Exception as ex:\n","        alert             = \"Danger\"\n","        alert_description = str(ex)\n","\n","        return (alert, alert_description, None)\n","    finally:\n","        if is_debug:\n","            par[\"locals\"] = locals()\n","            if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","            fn_print_debug_info(alert, fn_name, par)\n","            del par\n","\n","        fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"1c5a8d65-4d5d-41c9-af02-9ed863acc47b"},{"cell_type":"markdown","source":["## Extract"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"28c10f0b-73e0-439a-84b0-2806ab7247c3"},{"cell_type":"code","source":["def fn_bronze_extract():\n","    fn_name        = stk()[0][3]\n","    caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","    if is_debug: par = {}\n","    \n","    try:\n","        # Prepare for extract\n","        #   Clean up lh_log.log\n","        #   Insert SimpleNamespace.global_parameter in \"lh_log.log\"\n","        #   Delete the tables in \"lh_bronze\"\n","        #   Create lookup \"server_alias - server_name\"\n","\n","        # Extract preparation\n","        dict_server_alias_server_name = fn_extract_preparation(list_technology)[2]\n","\n","        if is_debug:\n","            print(f\"dict_server_alias_server_name (type): {type(dict_server_alias_server_name)}\")\n","            print(f\"dict_server_alias_server_name:        {dict_server_alias_server_name}\")\n","\n","        # Extract\n","        for row in sdf_technology_frequency.filter(sdf_technology_frequency.technology == \"SQL Server\").collect():\n","            fn_extract_lakehouse_or_sql_server(row.technology, row.frequency, dict_server_alias_server_name)\n","        \n","        for row in sdf_technology_frequency.filter(sdf_technology_frequency.technology == \"Lakehouse\").collect():\n","            fn_extract_lakehouse_or_sql_server(row.technology, row.frequency, dict_server_alias_server_name)\n","        \n","        for row in sdf_technology_frequency.filter(sdf_technology_frequency.technology == \"Excel\").collect():\n","            fn_extract_excel(row.technology, row.frequency)\n","        \n","        for row in sdf_technology_frequency.filter(sdf_technology_frequency.technology == \"CSV\").collect():\n","            fn_extract_csv(row.technology, row.frequency)\n","        \n","        for row in sdf_technology_frequency.filter(sdf_technology_frequency.technology == \"JSON\").collect():\n","            fn_extract_json(row.technology, row.frequency)\n","        \n","        #for row in sdf_technology_frequency.filter(sdf_technology_frequency.technology == \"API\").collect():\n","        #    fn_extract_api(row.technology, row.frequency)\n","\n","        # Extract finalization\n","        # 1. Append to table \"lh_log.extract\"\n","        #fn_append_to_lh_log_extract() PL: Do this or generate report from lh_log.log?\n","\n","        # 2. Update \"datetime_from\" and \"datetime_to\" in table \"lh_cfg.extract_parameter\" for the successfully extracted tables\n","        if \"SQL Server\" or \"Lakehouse\" in list_technology: fn_update_datetime_from_datetime_to_in_extract_parameter(pdf_log) # or \"API\"\n","\n","        alert             = \"Success\"\n","        alert_description = \"\"\n","\n","        # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","        return (alert, alert_description)\n","    except Exception as ex:\n","        alert             = \"Danger\"\n","        alert_description = str(ex)\n","\n","        return (alert, alert_description)\n","    finally:\n","        if is_debug:\n","            par[\"locals\"] = locals()\n","            fn_print_debug_info(alert, fn_name, par)\n","        \n","        # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","        \n","        # Append local log to lh_log.log\n","        fn_lh_log_log_insert()\n","\n","        # Generate HTML table to be sent by email to global_parameter.email_on_error\n","        #fn_get_extract_report()\n","\n","        return (alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f337ec51-420a-4d2b-81c6-42f3284220d8"},{"cell_type":"code","source":["if is_debug: print(\"\\n---------- This notenook name: nb_bronze_function - End ----------\\n\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"50e1b89d-d184-4d5b-a212-612ca9887c59"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"state":{},"version":"0.1"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}}},"nbformat":4,"nbformat_minor":5}