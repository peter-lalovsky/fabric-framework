{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edfc3ac0-c951-4bae-a20f-09357e91f276",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d934976-0931-4515-9d4f-7d016a323682",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if is_debug: print(\"\\n---------- This notenook name: nb_bronze_function_extract_csv - Start ----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6e7d6-ee9e-4b6c-91be-dd318761e624",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "medallion_name = \"Bronze\"\n",
    "if is_debug: print(f\"medallion_name: {medallion_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ffb93d-6cc7-40de-a7f6-5e955c9ef2f0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Current session info\n",
    "if is_debug: display(ss.getActiveSession())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ae9a7-89c9-4c59-bbd1-b56d1825890f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f38ef76-af49-4c2b-8a4a-a37d89cd4226",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Read CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44928246-93f5-40c3-92e1-34c9541a0df8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if \"CSV\" in list_technology:\n",
    "    def fn_read_csv_file(dict_parameter):\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        if is_debug: par = {}\n",
    "\n",
    "        try:\n",
    "            if dict_parameter[\"does_csv_file_exist\"] == True:\n",
    "                # Replace placeholders r, n, rn with \\r, \\n, \\r\\n\n",
    "                match dict_parameter[\"row_separator\"]:\n",
    "                    case 'r':  row_separator = \"\\r\"\n",
    "                    case 'n':  row_separator = \"\\n\"\n",
    "                    case 'rn': row_separator = \"\\r\\n\"\n",
    "                    case _:    dict_parameter[\"row_separator\"]\n",
    "                \n",
    "                options = {\"header\": dict_parameter[\"has_header\"]\n",
    ", \"delimiter\": dict_parameter[\"delimiter\"]\n",
    ", \"lineSep\": row_separator\n",
    ", \"quote\": dict_parameter[\"quote\"]\n",
    ", \"ignoreLeadingWhiteSpace\": \"True\"}\n",
    "\n",
    "                # Read CSV file\n",
    "                sdf = spark.read \\\n",
    "                    .options(**options) \\\n",
    "                    .csv(dict_parameter[\"csv_file_path_clean\"])\n",
    "                sdf_count = sdf.count()\n",
    "\n",
    "                # Apply the data types from \"global_parameter.projet_documentation_data_file_name\"\n",
    "                sdf_col    = fn_get_extract_column(dict_parameter)[2]\n",
    "                sdf_col    = sdf_col.withColumn(\"alias_name\", fn_clean_accent_in_sdf_column(\"column_name\")[2])\n",
    "                sdf_col    = fn_replace_all_not_alphanumericals_in_single_column_in_sdf(sdf_col, \"alias_name\", \"_\")[2]\n",
    "                select_col = fn_get_select_column(\"sdf\", sdf_col)[2]                \n",
    "                sdf        = eval(str(select_col))\n",
    "            else:\n",
    "                alert             = \"Warning\"\n",
    "                alert_description = f\"CSV file \\\"{csv_file_path}\\\" does not exist.\"\n",
    "\n",
    "            alert             = \"Success\"\n",
    "            alert_description = f\"CSV file (rows): {sdf_count}\"\n",
    "\n",
    "            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n",
    "\n",
    "            return (alert, alert_description, sdf, sdf_count)                \n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            return (alert, alert_description, None, None)\n",
    "        finally:\n",
    "            if is_debug:\n",
    "                par[\"locals\"] = locals()\n",
    "                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n",
    "                fn_print_debug_info(alert, fn_name, par)\n",
    "                del par\n",
    "\n",
    "        fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d169600-a708-4269-a1ab-2df061968d51",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14477d3-c013-4743-8be5-22ff3fc678d2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Extract CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5400f03-d217-40ac-b585-04245c669a61",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if \"CSV\" in list_technology:\n",
    "    def fn_extract_csv(\n",
    "        technology\n",
    "        , frequency\n",
    "    ):\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        if is_debug: par = {}\n",
    "\n",
    "        try:\n",
    "            # Rename the existing files in lh_bronze\n",
    "            sdf_renamed = fn_rename_files_in_lh_bronze(technology, frequency, \"Archive\")[1]\n",
    "\n",
    "            # Get extract table list\n",
    "            dict_parameter_table_list     = {\"technology\": technology, \"frequency\": frequency, \"action\": \"Extract\"}\n",
    "            rv_extract_table_list         = fn_get_extract_table_list(dict_parameter_table_list)\n",
    "            sdf_extract_table_list_status = rv_extract_table_list[0]\n",
    "            sdf_extract_table_list        = rv_extract_table_list[2]\n",
    "            sdf_extract_table_list_count  = rv_extract_table_list[3]\n",
    "\n",
    "            if is_debug:\n",
    "                print(f\"sdf_extract_table_list_status: {sdf_extract_table_list_status}\")\n",
    "                print(f\"sdf_extract_table_list:\")\n",
    "                display(sdf_extract_table_list)\n",
    "                print(f\"sdf_extract_table_list_count:  {sdf_extract_table_list_count}\")\n",
    "\n",
    "            # Loop extract table list\n",
    "            if sdf_extract_table_list_count > 0:\n",
    "                # etl_r --> sdf_extract_table_list.row\n",
    "                for etl_r in sdf_extract_table_list \\\n",
    "                    .sort(sdf_extract_table_list.technology.asc(),\n",
    "                        sdf_extract_table_list.frequency.asc(),                        \n",
    "                        sdf_extract_table_list.folder_name.asc(),\n",
    "                        sdf_extract_table_list.file_name.asc()\n",
    "                    ).collect():\n",
    "\n",
    "                    dict_parameter = {\"process_timestamp\": global_parameter.process_timestamp\n",
    ", \"technology\": etl_r.technology\n",
    ", \"frequency\": etl_r.frequency\n",
    ", \"folder_name\": etl_r.folder_name\n",
    ", \"file_name\": etl_r.file_name\n",
    ", \"has_header\": etl_r.has_header\n",
    ", \"delimiter\": etl_r.delimiter\n",
    ", \"row_separator\": etl_r.row_separator\n",
    ", \"quote\": etl_r.quote}\n",
    "\n",
    "                    if is_debug:\n",
    "                        print(f\"sdf_extract_table_list.etl_r:\")\n",
    "                        print(f\"  technology:    {etl_r.technology}\")\n",
    "                        print(f\"  frequency:     {etl_r.frequency}\")\n",
    "                        print(f\"  folder_name:   {etl_r.folder_name}\")\n",
    "                        print(f\"  file_name:     {etl_r.file_name}\")                    \n",
    "                        print(f\"  has_header:    {etl_r.has_header}\")\n",
    "                        print(f\"  delimiter:     {etl_r.delimiter}\")\n",
    "                        print(f\"  row_separator: {etl_r.row_separator}\")\n",
    "                        print(f\"  quote:         {etl_r.quote}\")\n",
    "                        print(f\"dict_parameter: {dict_parameter}\")\n",
    "                    \n",
    "                    # Clean up \"technology\", \"folder_name\", \"file_name\" - Start\n",
    "                    # Clean accents and not alphanumerics\n",
    "                    technology_clean  = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(technology)[2], \"_\")[2]\n",
    "                    folder_name_clean = etl_r.folder_name.replace(\"/\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\") # keep \"/\" while cleaning\n",
    "                    folder_name_clean = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(folder_name_clean)[2], \"_\")[2]\n",
    "                    folder_name_clean = folder_name_clean.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"/\")\n",
    "                    file_name_clean   = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(etl_r.file_name)[2], \"_\")[2]\n",
    "\n",
    "                    dict_parameter[\"technology_clean\"]  = technology_clean\n",
    "                    dict_parameter[\"folder_name_clean\"] = folder_name_clean\n",
    "                    dict_parameter[\"file_name_clean\"]   = file_name_clean\n",
    "\n",
    "                    if is_debug:\n",
    "                        print(f\"folder_name_clean: >{folder_name_clean}<\")\n",
    "                        print(f\"file_name_clean: >{file_name_clean}<\")\n",
    "                        print(f\"dict_parameter: {dict_parameter}\")\n",
    "                    # Clean up \"technology\", \"folder_name\", \"file_name\" - End\n",
    "\n",
    "                    # Generate source file path and check if file exists - Start\n",
    "                    csv_file_path_clean = f\"{global_parameter.abfs_path_lh_bronze}/Files/incoming/{etl_r.technology}/{folder_name_clean}/{file_name_clean}.csv\"\n",
    "                    csv_file_path_clean = csv_file_path_clean.replace(\"abfss://\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n",
    "                    csv_file_path_clean = csv_file_path_clean.replace(\"//\", \"/\")\n",
    "                    csv_file_path_clean = csv_file_path_clean.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"abfss://\")\n",
    "\n",
    "                    does_csv_file_exist = fn_does_file_exist(csv_file_path_clean, dict_parameter)[2]\n",
    "\n",
    "                    dict_parameter[\"csv_file_path_clean\"] = csv_file_path_clean\n",
    "                    dict_parameter[\"does_csv_file_exist\"] = does_csv_file_exist\n",
    "\n",
    "                    if is_debug:\n",
    "                        print(f\"csv_file_path_clean: {csv_file_path_clean}\")\n",
    "                        print(f\"does_csv_file_exist: {does_csv_file_exist}\")\n",
    "                        print(f\"dict_parameter: {dict_parameter}\")\n",
    "                    # Generate source file path and check if file exists - End\n",
    "                    \n",
    "                    if does_csv_file_exist == True:\n",
    "                        # Extract file - Start\n",
    "                        rv_csv_file         = fn_read_csv_file(dict_parameter)\n",
    "                        sdf_csv_file_status = rv_csv_file[0]\n",
    "                        sdf_csv_file        = rv_csv_file[2]\n",
    "                        sdf_csv_file_count  = rv_csv_file[3]\n",
    "\n",
    "                        if is_debug:\n",
    "                            print(f\"rv_csv_file:         {rv_csv_file}\")\n",
    "                            print(f\"sdf_csv_file_status: {sdf_csv_file_status}\")\n",
    "                            print(f\"sdf_csv_file (schema):\")\n",
    "                            sdf_csv_file.printSchema()\n",
    "                            print(f\"sdf_csv_file:\")\n",
    "                            display(sdf_csv_file)\n",
    "                            print(f\"sdf_csv_file_count:  {sdf_csv_file_count}\")\n",
    "                        # Extract file - End\n",
    "\n",
    "                        # Save CSV file as table in \"lh_bronze\" - Start\n",
    "                        # Replace \"/\" with \"_\" in \"folder_name\"\n",
    "                        folder_name_in_table_name = folder_name_clean.replace(\"/\", \"_\")\n",
    "\n",
    "                        # Generate full table path\n",
    "                        csv_file_table_path = f\"{global_parameter.abfs_path_lh_bronze}/Tables/{technology_clean}_{folder_name_in_table_name}_{file_name_clean}\"\n",
    "\n",
    "                        # Remove double underscore (folder_name is empty)\n",
    "                        csv_file_table_path = csv_file_table_path.replace(f\"__\", \"_\")\n",
    "\n",
    "                        if is_debug: print(f\"sdf_csv_file_table_path: {csv_file_table_path}\")\n",
    "\n",
    "                        fn_save_sdf_as_table(\n",
    "                            sdf_csv_file\n",
    "                            , \"delta\"\n",
    "                            , \"overwrite\"\n",
    "                            , csv_file_table_path\n",
    "                            , dict_parameter\n",
    "                        )\n",
    "                        # Save CSV file as table in \"lh_bronze\" - End\n",
    "\n",
    "                        # Copy and delete CSV file to \"lh_bronze/Processed/CSV\" (archive) - Start\n",
    "                        now_year  = fn_get_now(\"year\")[1]\n",
    "                        now_month = fn_get_now(\"month\")[1]\n",
    "                        dstn_file_path = f\"{global_parameter.abfs_path_lh_bronze}/Files/processed/{technology_clean}/\\\n",
    "{folder_name_clean}/{file_name_clean}/\\\n",
    "{now_year}/{now_month}/\\\n",
    "{file_name_clean}_{global_parameter.process_timestamp}.csv\"\n",
    "\n",
    "                        dstn_file_path = dstn_file_path.replace(\"abfss://\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n",
    "                        dstn_file_path = dstn_file_path.replace(\"//\", \"/\")\n",
    "                        dstn_file_path = dstn_file_path.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"abfss://\")\n",
    "\n",
    "                        copy_file_status = fn_copy_file(csv_file_path_clean, dstn_file_path, dict_parameter)[0]\n",
    "                        if copy_file_status == \"Success\": fn_delete_file(csv_file_path_clean, dict_parameter)\n",
    "\n",
    "                        if is_debug:\n",
    "                            print(f\"csv_file_path_clean: {csv_file_path_clean}\")\n",
    "                            print(f\"dstn_file_path: {dstn_file_path}\")\n",
    "                        # Copy and delete CSV file to \"lh_bronze/Processed/CSV\" (archive) - End\n",
    "\n",
    "                        # Insert \"Success\" in lh_log (for each file) - Start\n",
    "                        # Always log \"Success\" for extracted object.\n",
    "                        alert             = \"Success\"\n",
    "                        alert_description = f\"Row count: {str(sdf_csv_file_count)}\"\n",
    "\n",
    "                        if is_debug:\n",
    "                            par[\"locals\"] = locals()\n",
    "                            fn_print_debug_info(alert, fn_name, par)\n",
    "                            del par\n",
    "\n",
    "                        fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)\n",
    "                        # Insert \"Success\" in lh_log (for each file) - End\n",
    "                        \n",
    "                        if is_debug: print(\"--------------- Loop - End ---------------\")\n",
    "\n",
    "            alert             = \"Success\"\n",
    "            alert_description = f\"\"\n",
    "\n",
    "            return (alert, alert_description)\n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            if is_debug:\n",
    "                par[\"locals\"] = locals()\n",
    "                fn_print_debug_info(alert, fn_name, par)\n",
    "                del par\n",
    "\n",
    "            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)\n",
    "\n",
    "            return (alert, alert_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb92bb-4c00-44ce-a6b4-2bc8ae8dfc0b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if is_debug: print(\"\\n---------- This notenook name: nb_bronze_function_extract_csv - End ----------\\n\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
