{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc64dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PL: Split SQl Server ffrom Lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c9662-f889-449b-a3d8-388b7a0cc360",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736f086-73d7-4dbd-b9b3-6fd062e409ad",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if is_debug: print(\"\\n---------- This notenook name: nb_bronze_function_extract_lakehouse_sql_server - Start ----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234db21-912d-40e2-a018-f82c6d161621",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "medallion_name = \"Bronze\"\n",
    "if is_debug: print(f\"medallion_name: {medallion_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce5bb0-b681-4a0f-b7aa-3a0af5cfda5c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Current session info\n",
    "if is_debug: display(ss.getActiveSession())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2288e24c-9bc6-4e08-a5af-f8eeed17ad41",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e01ad-1f89-4e80-b207-a4badf34b9f5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get \"server_name\" for \"server_alias\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2d123-ac8f-402d-8e59-dd28d3fdf993",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Used to apply \"SQL Server aliases\"\n",
    "if \"SQL Server\" in list_technology:    \n",
    "    def fn_get_server_name_for_server_alias(\n",
    "        dict_server_alias_server_name\n",
    "        , server_alias\n",
    "    ):\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        if is_debug: par = {}\n",
    "        \n",
    "        try:\n",
    "            server_name = None\n",
    "\n",
    "            for key, value in dict_server_alias_server_name.items():\n",
    "                if key == server_alias:\n",
    "                    server_name = value\n",
    "                    break\n",
    "\n",
    "            alert             = \"Success\"\n",
    "            alert_description = f\"Server alias - server name: {server_alias} - {server_name}\"\n",
    "\n",
    "            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n",
    "\n",
    "            return (alert, alert_description, server_name)\n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            return (alert, alert_description, None)\n",
    "        finally:\n",
    "            if is_debug:\n",
    "                par[\"locals\"] = locals()\n",
    "                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n",
    "                fn_print_debug_info(alert, fn_name, par)\n",
    "                del par\n",
    "\n",
    "            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feae575d-70b0-4189-afbf-f0086e2ca8f4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Execute SQL query (on-premises SQL Server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35601a9-ad38-47d9-b9e2-9a9eddaf4955",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# On-Premises SQL Server, not behind VPN\n",
    "# PL: Will be reaplced with shortcuts\n",
    "if \"SQL Server\" in list_technology:\n",
    "    def fn_execute_sql_query(dict_parameter):\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        if is_debug: par = {}\n",
    "\n",
    "        try:\n",
    "            username      = notebookutils.credentials.getSecret(dict_parameter[\"keyvault_url\"], dict_parameter[\"keyvault_secret_name_user\"])\n",
    "            password      = notebookutils.credentials.getSecret(dict_parameter[\"keyvault_url\"], dict_parameter[\"keyvault_secret_name_password\"])\n",
    "            server_name   = dict_parameter[\"server_name\"]\n",
    "            database_name = dict_parameter[\"database_name\"]\n",
    "\n",
    "            sdf = spark.read\\\n",
    "              .format(\"jdbc\")\\\n",
    "              .option(\"url\"                , f\"jdbc:sqlserver://{server_name};databaseName={database_name}\")\\\n",
    "              .option(\"query\"              , dict_parameter[\"sql_query\"])\\\n",
    "              .option(\"mssqlIsolationLevel\", global_parameter.mssql_isolation_level)\\\n",
    "              .option(\"user\"               , username)\\\n",
    "              .option(\"password\"           , password)\\\n",
    "              .option(\"driver\"             , \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "              .load()\n",
    "            sdf_count = sdf.count()\n",
    "\n",
    "            alert             = \"Success\"\n",
    "            alert_description = f\"Execute SQL Query (count): {sdf_count}\"\n",
    "\n",
    "            return (alert, alert_description, sdf, sdf_count)\n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            return (alert, alert_description, None, 0)\n",
    "        finally:\n",
    "            if is_debug:\n",
    "                par[\"locals\"] = locals()\n",
    "                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n",
    "                fn_print_debug_info(alert, fn_name, par)\n",
    "                del par\n",
    "\n",
    "            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c647d0-39b9-4baa-a862-329cdf5bf865",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Add initial values in table \"lh_cfg.extract_parameter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96810282-31ce-4a5a-8262-05ff39872b5f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if \"SQL Server\" in list_technology or \"Lakehouse\" in list_technology:\n",
    "    def fn_add_initial_extract_parameter(\n",
    "        technology\n",
    "        , frequency\n",
    "    ):\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        if is_debug: par = {}\n",
    "\n",
    "        try:\n",
    "            # Edit this query to cover your \"datetime_from_to\" filters\n",
    "            sql_code = f\"\"\"SELECT\n",
    "    \"{technology}\"            AS `technology`\n",
    "    , \"{frequency}\"           AS `frequency`\n",
    "    , EO.`lakehouse_name`\n",
    "    , EO.`server_name`\n",
    "    , EO.`database_name`\n",
    "    , EO.`schema_name`\n",
    "    , EO.`table_name`\n",
    "    , \"datetime_from\"           AS `name`\n",
    "    , \"1900-01-01 00:00:00.000\" AS `value`\n",
    "FROM\n",
    "    (\n",
    "              SELECT `technology`, `frequency`, `server_name`, `database_name`, `schema_name`, `table_name`, \\\"\\\" AS `lakehouse_name`, `sequence`, `prefix_select`, `column_name`, `from`, `where`, `is_pk`, `data_type`, `is_extracted`                 FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_sql_server`\n",
    "        UNION SELECT `technology`, `frequency`, \\\"\\\" AS `server_name`, \\\"\\\" AS `database_name`, \\\"\\\" AS `schema_name`, `table_name`, `lakehouse_name`, `sequence`, `prefix_select`, `column_name`, `from`, `where`, `is_pk`, `data_type`, `is_extracted` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_lakehouse`\n",
    "    )                                                                              AS EO\n",
    "    LEFT JOIN delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_parameter` AS EP\n",
    "        ON EO.`technology`                  = EP.`technology`\n",
    "        AND EO.`frequency`                  = EP.`frequency`\n",
    "        AND IFNULL(EO.`lakehouse_name`, \"\") = IFNULL(EP.`lakehouse_name`, \"\")\n",
    "        AND IFNULL(EO.`server_name`, \"\")    = IFNULL(EP.`server_name`, \"\")\n",
    "        AND IFNULL(EO.`database_name`, \"\")  = IFNULL(EP.`database_name`, \"\")\n",
    "        AND IFNULL(EO.`schema_name`, \"\")    = IFNULL(EP.`schema_name`, \"\")\n",
    "        AND IFNULL(EO.`table_name`, \"\")     = IFNULL(EP.`table_name`, \"\")\n",
    "WHERE\n",
    "    EP.`technology`       IS NULL\n",
    "    AND EO.`technology`   = \\\"{technology}\\\"\n",
    "    AND EO.`frequency`    = \\\"{frequency}\\\"\n",
    "    AND EO.`is_extracted` = 1\n",
    "    AND EO.`where`        LIKE \"%^datetime_from^%\"\n",
    "\n",
    "UNION ALL SELECT\n",
    "    \\\"{technology}\\\"      AS `technology`\n",
    "    , \\\"{frequency}\\\"     AS `frequency`\n",
    "    , EO.`lakehouse_name`\n",
    "    , EO.`server_name`\n",
    "    , EO.`database_name`\n",
    "    , EO.`schema_name`\n",
    "    , EO.`table_name`\n",
    "    , \"datetime_to\"       AS `name`\n",
    "    , NULL                AS `value`\n",
    "FROM\n",
    "    (\n",
    "              SELECT `technology`, `frequency`, `server_name`, `database_name`, `schema_name`, `table_name`, \\\"\\\" AS `lakehouse_name`, `sequence`, `prefix_select`, `column_name`, `from`, `where`, `is_pk`, `data_type`, `is_extracted`                 FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_sql_server`\n",
    "        UNION SELECT `technology`, `frequency`, \\\"\\\" AS `server_name`, \\\"\\\" AS `database_name`, \\\"\\\" AS `schema_name`, `table_name`, `lakehouse_name`, `sequence`, `prefix_select`, `column_name`, `from`, `where`, `is_pk`, `data_type`, `is_extracted` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_lakehouse`\n",
    "    )                                                                              AS EO\n",
    "    LEFT JOIN delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_parameter` AS EP\n",
    "        ON EO.`technology`              = EP.`technology`\n",
    "        AND EO.`frequency`                  = EP.`frequency`\n",
    "        AND IFNULL(EO.`lakehouse_name`, \"\") = IFNULL(EP.`lakehouse_name`, \"\")\n",
    "        AND IFNULL(EO.`server_name`, \"\")    = IFNULL(EP.`server_name`, \"\")\n",
    "        AND IFNULL(EO.`database_name`, \"\")  = IFNULL(EP.`database_name`, \"\")\n",
    "        AND IFNULL(EO.`schema_name`, \"\")    = IFNULL(EP.`schema_name`, \"\")\n",
    "        AND IFNULL(EO.`table_name`, \"\")     = IFNULL(EP.`table_name`, \"\")\n",
    "WHERE\n",
    "    EP.`technology`       IS NULL\n",
    "    AND EO.`technology`   = \\\"{technology}\\\"\n",
    "    AND EO.`frequency`    = \\\"{frequency}\\\"\n",
    "    AND EO.`is_extracted` = 1\n",
    "    AND EO.`where`        LIKE \"%^datetime_to^%\"\n",
    "ORDER BY\n",
    "    `technology`\n",
    "    , `frequency`\n",
    "    , `lakehouse_name`\n",
    "    , `server_name`\n",
    "    , `database_name`\n",
    "    , `schema_name`\n",
    "    , `table_name`\n",
    "    , `name`;\"\"\"\n",
    "\n",
    "            rv_initial_extract_parameter = fn_execute_spark_sql(sql_code)\n",
    "            sdf                          = rv_initial_extract_parameter[2]\n",
    "            sdf_count                    = rv_initial_extract_parameter[3]\n",
    "\n",
    "            if sdf_count > 0:\n",
    "                # PL: \n",
    "                # Debug this - Start\n",
    "                # Bug: Uses another sdf (the one from the \"Test\" section of fn_save_sdf_as_table())\n",
    "                #fn_save_sdf_as_table(\"sdf\", \\\n",
    "                #    \"delta\", \\\n",
    "                #    \"append\", \\\n",
    "                #    f\"{global_parameter.abfs_path_lh_cfg}/Tables/extract_parameter\", \\\n",
    "                #    None)\n",
    "\n",
    "                sdf.write\\\n",
    "                    .format(\"delta\")\\\n",
    "                    .mode(\"append\")\\\n",
    "                    .save(f\"{global_parameter.abfs_path_lh_cfg}/Tables/extract_parameter\")\n",
    "                # Debug this - End\n",
    "\n",
    "            alert             = \"Success\"\n",
    "            alert_description = f\"New rows (count): {str(sdf_count)}\"\n",
    "\n",
    "            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n",
    "\n",
    "            return (alert, alert_description, sdf_count)\n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            return (alert, alert_description, None)\n",
    "        finally:\n",
    "            if is_debug:\n",
    "                par[\"locals\"] = locals()\n",
    "                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n",
    "                fn_print_debug_info(alert, fn_name, par)\n",
    "                del par\n",
    "\n",
    "            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfbfbd3-9de5-45c5-8a31-53f10b918bfb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Update value \"datetime_to\" in table \"extract_parameter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db42645-4d4d-44c7-904b-2e8e97924a7e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Before the execution of the extraction loop:\n",
    "#    • extraction_timeframe = Now --> datetime_to = now()\n",
    "#    • extraction_timeframe IN (Previous Full Day,\n",
    "#       Previous Full Week, Previous Full Month,\n",
    "#       Previous Full Quarter, Previous Full Year) --> datetime_to = Today at 00:00:00.000\n",
    "if \"SQL Server\" in list_technology or \"Lakehouse\" in list_technology:\n",
    "    def fn_update_datetime_to_in_extract_parameter(technology, frequency):\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        if is_debug: par = {}\n",
    "\n",
    "        row        = None\n",
    "        sdf_list   = None\n",
    "        pdf_list   = None\n",
    "        rv_update  = None\n",
    "        sdf_update = None\n",
    "\n",
    "        try:\n",
    "            # Get all \"extraction_timeframe\"\n",
    "            sql_code_extraction_timeframe = f\"\"\"SELECT DISTINCT `extraction_timeframe`\n",
    "FROM\n",
    "    (\n",
    "              SELECT `technology`, `frequency`, `where`, `is_extracted`, `extraction_timeframe` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_sql_server`\n",
    "        UNION SELECT `technology`, `frequency`, `where`, `is_extracted`, `extraction_timeframe` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_lakehouse`\n",
    "    )\n",
    "WHERE\n",
    "    `technology`       = \\\"{technology}\\\"\n",
    "    AND `frequency`    = \\\"{frequency}\\\"\n",
    "    AND `where`        LIKE \\\"%datetime_to%\\\"\n",
    "    AND `is_extracted` = 1;\"\"\"\n",
    "\n",
    "            rv_extraction_timeframe        = fn_execute_spark_sql(sql_code_extraction_timeframe)\n",
    "            sdf_extraction_timeframe       = rv_extraction_timeframe[2]\n",
    "            sdf_extraction_timeframe_count = rv_extraction_timeframe[3]\n",
    "\n",
    "            dict_count = {}\n",
    "            count      = 0\n",
    "\n",
    "            # If \"extraction_timeframe\" exists\n",
    "            if sdf_extraction_timeframe_count > 0:\n",
    "                sql_code_list = f\"\"\"SELECT CONCAT(\n",
    "    `technology`, '|'\n",
    "    , `frequency`, '|'\n",
    "    , IFNULL(`lakehouse_name`, \\\"\\\"), '|'\n",
    "    , IFNULL(`server_name`   , \\\"\\\"), '|'\n",
    "    , IFNULL(`database_name` , \\\"\\\"), '|'\n",
    "    , IFNULL(`schema_name`   , \\\"\\\"), '|'\n",
    "    , IFNULL(`table_name`    , \\\"\\\")\n",
    ") AS I\n",
    "FROM\n",
    "    (\n",
    "              SELECT `technology`, `frequency`, '' AS `lakehouse_name`, `server_name`, `database_name`, `schema_name`, `table_name`, `where`, `extraction_timeframe`, `is_extracted`             FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_sql_server`\n",
    "        UNION SELECT `technology`, `frequency`, `lakehouse_name`, '' AS `server_name`, '' AS `database_name`, '' AS `schema_name`, `table_name`, `where`, `extraction_timeframe`, `is_extracted` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_lakehouse`\n",
    "    )\n",
    "WHERE\n",
    "    `technology`               = \\\"{technology}\\\"\n",
    "    AND `frequency`            = \\\"{frequency}\\\"\n",
    "    AND `where`                LIKE '%datetime_to%'\n",
    "    AND `extraction_timeframe` = \\\"^extraction_timeframe^\\\"\n",
    "    AND `is_extracted`         = 1;\"\"\"\n",
    "\n",
    "                sql_code_update = f\"\"\"UPDATE delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_parameter`\n",
    "SET `value` = \\\"^value^\\\"\n",
    "WHERE\n",
    "    CONCAT(\n",
    "        `technology`, '|',\n",
    "        `frequency`, '|',\n",
    "        IFNULL(`lakehouse_name`, \\\"\\\"), '|',\n",
    "        IFNULL(`server_name`   , \\\"\\\"), '|',\n",
    "        IFNULL(`database_name` , \\\"\\\"), '|',\n",
    "        IFNULL(`schema_name`   , \\\"\\\"), '|',\n",
    "        IFNULL(`table_name`    , \\\"\\\")\n",
    "    ) IN (^list^)\n",
    "    AND `name` LIKE \\\"%datetime_to%\\\"\n",
    "    AND `frequency` = \\\"{frequency}\\\";\"\"\"\n",
    "\n",
    "                # Loop \"extraction_timeframe\" for parameter \"frequency\"\n",
    "                for row in sdf_extraction_timeframe.collect():\n",
    "                    if is_debug: print(f\"\\n----- extraction_timeframe = \\\"{row.extraction_timeframe}\\\" - Start -----\\n\")\n",
    "                    \n",
    "                    sql_code_list  = sql_code_list.replace(\"^extraction_timeframe^\", row.extraction_timeframe)\n",
    "                    rv_list        = fn_execute_spark_sql(sql_code_list)\n",
    "                    sdf_list       = rv_list[2]\n",
    "                    sdf_list_count = rv_list[3]\n",
    "\n",
    "                    if sdf_list_count > 0:\n",
    "                        # PL: Define all the cases\n",
    "                        if row.extraction_timeframe == \"Now\": value = fn_get_now(\"datetime\")[1]\n",
    "                        \n",
    "                        if row.extraction_timeframe in (\"Previous Full Day\",\n",
    "                            \"Previous Full Day\",\n",
    "                            \"Previous Full Week\",\n",
    "                            \"Previous Full Month\",\n",
    "                            \"Previous Full Quarter\",\n",
    "                            \"Previous Full Year\"):\n",
    "                            value = f\"{fn_get_now('date')[1]} 00:00:00.000\"\n",
    "                        \n",
    "                        pdf_list = sdf_list.toPandas()\n",
    "                        rv_list  = str(pdf_list['I'].tolist()).replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "                        sql_code_update = sql_code_update.replace(\"^value^\", value)\n",
    "                        sql_code_update = sql_code_update.replace(\"^list^\", rv_list)\n",
    "\n",
    "                        rv_update        = fn_execute_spark_sql(sql_code_update)\n",
    "                        sdf_update       = rv_update[2]\n",
    "                        sdf_update_count = rv_update[3]\n",
    "\n",
    "                        sdf_update_count                     = sdf_update.collect()[0][0]\n",
    "                        dict_count[row.extraction_timeframe] = sdf_update_count\n",
    "                        count                                += sdf_update_count\n",
    "\n",
    "                    if is_debug: print(f\"\\n----- extraction_timeframe = \\\"{row.extraction_timeframe}\\\" - End -----\\n\")\n",
    "\n",
    "            alert             = \"Success\"\n",
    "            alert_description = f\"Updated rows (count): {str(count)}\"\n",
    "\n",
    "            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n",
    "\n",
    "            return (alert, alert_description, dict_count, count)\n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            return (alert, alert_description, None, None)\n",
    "        finally:\n",
    "            if is_debug:\n",
    "                par[\"locals\"] = locals()\n",
    "                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n",
    "                fn_print_debug_info(alert, fn_name, par)\n",
    "                del par\n",
    "\n",
    "            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b06183e-1fc9-4854-bbd5-fc1f7901fdbb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Generate SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f19c74-23c2-49bd-bdea-7c40ec5eeafe",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if \"SQL Server\" in list_technology or \"Lakehouse\" in list_technology:\n",
    "    def fn_get_statament_select(\n",
    "        sdf\n",
    "        , technology\n",
    "    ):\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        if is_debug: par = {}\n",
    "\n",
    "        try:\n",
    "            statament_select = f\"SELECT \"\n",
    "\n",
    "            if technology == \"SQL Server\":\n",
    "                for row in sdf.sort(sdf.sequence.asc()).collect():\n",
    "                    statament_select += row.alias\n",
    "            \n",
    "            if technology == \"Lakehouse\":\n",
    "                for row in sdf.sort(sdf.sequence.asc()).collect():\n",
    "                    statament_select += f\"`{row.column_name }`, \"\n",
    "                \n",
    "                statament_select = statament_select[:-2]\n",
    "\n",
    "            alert             = \"Success\"\n",
    "            alert_description = f\"\"\n",
    "\n",
    "            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n",
    "\n",
    "            return (alert, alert_description, statament_select)\n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            return (alert, alert_description, None)\n",
    "        finally:\n",
    "            if is_debug:\n",
    "                par[\"locals\"] = locals()\n",
    "                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n",
    "                fn_print_debug_info(alert, fn_name, par)\n",
    "                del par\n",
    "\n",
    "            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920d2ef-b777-4861-89a1-ab855e1b7664",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Generate FROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd7b2a-a40d-4b53-92de-b5d6adf59a36",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if \"SQL Server\" in list_technology or \"Lakehouse\" in list_technology:\n",
    "    def fn_get_statament_from(\n",
    "        sdf\n",
    "        , lakehouse_name\n",
    "        , schema_name\n",
    "        , table_name\n",
    "        , technology\n",
    "    ):\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        if is_debug: par = {}\n",
    "\n",
    "        try:\n",
    "            from_ = sdf.collect()[0][\"from\"]\n",
    "\n",
    "            if from_ == \"\":\n",
    "                if technology == \"Lakehouse\":\n",
    "                    path = f\"global_parameter.abfs_path_{lakehouse_name}\"\n",
    "                    statament_from = f\"FROM delta.`{eval(path)}/Tables/{table_name}`\"\n",
    "                elif technology == \"SQL Server\":\n",
    "                    statament_from = f\"FROM [{schema_name}].[{table_name}]\"\n",
    "            else:\n",
    "                if technology == \"Lakehouse\":\n",
    "                    statament_from = f\"FROM {from_}\"\n",
    "                    # replace \"table_name\" in \"from_\" with f\"FROM delta.`{global_parameter.abfs_path_lh}_{lakehouse_name}/Tables/{table_name}`\"\n",
    "                elif technology == \"SQL Server\":\n",
    "                    statament_from = f\"FROM {from_}\"\n",
    "\n",
    "            alert             = \"Success\"\n",
    "            alert_description = f\"\"\n",
    "\n",
    "            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n",
    "\n",
    "            return (alert, alert_description, statament_from)\n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            return (alert, alert_description, None)\n",
    "        finally:\n",
    "            if is_debug:\n",
    "                par[\"locals\"] = locals()\n",
    "                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n",
    "                fn_print_debug_info(alert, fn_name, par)\n",
    "                del par\n",
    "\n",
    "            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fa8dc-8c0b-47ef-815c-b1c1b1ba2b05",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Generate WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498be1cd-9045-4a6e-89c6-d5c45f4ca1d9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if \"SQL Server\" in list_technology or \"Lakehouse\" in list_technology:\n",
    "    def fn_get_statement_where(\n",
    "        sdf\n",
    "        , dict_parameter\n",
    "    ):\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        if is_debug: par = {}\n",
    "\n",
    "        try:\n",
    "            statement_where = \"WHERE \"\n",
    "            where_count     = sdf.filter(sf.col(\"where\") != \"\").count()\n",
    "\n",
    "            if where_count > 0:\n",
    "                sql_code = f\"\"\"SELECT A.`where`\n",
    "FROM\n",
    "    (\n",
    "        SELECT `technology`, `frequency`, '' AS `lakehouse_name`, `server_name`, `database_name`, `schema_name`, `table_name`, `where`, `is_extracted`                   FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_sql_server`\n",
    "        UNION SELECT `technology`, `frequency`, `lakehouse_name`, '' AS `server_name`, '' AS `database_name`, '' AS `schema_name`, `table_name`, `where`, `is_extracted` FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_lakehouse`\n",
    "    ) AS A\n",
    "WHERE\n",
    "    A.`technology`                       = \\\"{dict_parameter[\"technology\"]}\\\"\n",
    "    AND A.`frequency`                    = \\\"{dict_parameter[\"frequency\"]}\\\"\n",
    "    AND IFNULL(A.`lakehouse_name`, \\\"\\\") = \\\"{dict_parameter[\"lakehouse_name\"]}\\\"\n",
    "    AND IFNULL(A.`server_name`   , \\\"\\\") = \\\"{dict_parameter[\"server_alias\"]}\\\"\n",
    "    AND IFNULL(A.`database_name` , \\\"\\\") = \\\"{dict_parameter[\"database_name\"]}\\\"\n",
    "    AND IFNULL(A.`schema_name`   , \\\"\\\") = \\\"{dict_parameter[\"schema_name\"]}\\\"\n",
    "    AND IFNULL(A.`table_name`    , \\\"\\\") = \\\"{dict_parameter[\"table_name\"]}\\\"\n",
    "    AND A.`where`                        IS NOT NULL\n",
    "    AND A.`is_extracted`                 = 1;\"\"\"\n",
    "\n",
    "                statement_where += fn_execute_spark_sql(sql_code)[2].collect()[0][0]\n",
    "\n",
    "                sql_code = f\"\"\"SELECT\n",
    "    `name`\n",
    "    , `value`\n",
    "FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_parameter`\n",
    "WHERE\n",
    "    `technology`                       = \\\"{dict_parameter[\"technology\"]}\\\"\n",
    "    AND `frequency`                    = \\\"{dict_parameter[\"frequency\"]}\\\"\n",
    "    AND IFNULL(`lakehouse_name`, \\\"\\\") = \\\"{dict_parameter[\"lakehouse_name\"]}\\\"\n",
    "    AND IFNULL(`server_name`   , \\\"\\\") = \\\"{dict_parameter[\"server_alias\"]}\\\"\n",
    "    AND IFNULL(`database_name` , \\\"\\\") = \\\"{dict_parameter[\"database_name\"]}\\\"\n",
    "    AND IFNULL(`schema_name`   , \\\"\\\") = \\\"{dict_parameter[\"schema_name\"]}\\\"\n",
    "    AND IFNULL(`table_name`    , \\\"\\\") = \\\"{dict_parameter[\"table_name\"]}\\\";\"\"\"\n",
    "\n",
    "                sdf_extract_parameter = fn_execute_spark_sql(sql_code)[2]\n",
    "\n",
    "                for row in sdf_extract_parameter.collect(): statement_where = statement_where.replace(f\"^{row.name}^\", f\"'{row.value}'\")\n",
    "\n",
    "            if statement_where == \"WHERE \": statement_where = \"\"\n",
    "\n",
    "            alert             = \"Success\"\n",
    "            alert_description = f\"\"\n",
    "\n",
    "            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n",
    "\n",
    "            return (alert, alert_description, statement_where)\n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            return (alert, alert_description, None)\n",
    "        finally:\n",
    "            if is_debug:\n",
    "                par[\"locals\"] = locals()\n",
    "                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n",
    "                fn_print_debug_info(alert, fn_name, par)\n",
    "                del par\n",
    "\n",
    "            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0f3a6-94c0-4155-99de-f7a6c6409d71",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" PL:\n",
    "fn_append_to_lh_log_extract\n",
    "    Add column \"status\" --> Use Logging Levels Debug, Info, Warn, Error, Fatal, instead of Bootstrap Success, Warning, Danger\n",
    "    Add code to break \"locals\" to fn_name and caller_fn_name and get \"Success\" from all parent - child functions\n",
    "\n",
    "fn_update_datetime_from_datetime_to_in_extract_parameter\n",
    "    Add code to break \"locals\" to fn_name and caller_fn_name and get \"Success\" from all parent - child functions\n",
    "    Example of failed extraction:\n",
    "        fn_name                             caller_fn_name                          alert\n",
    "        *******                             **************                          *****\n",
    "        fn_extract_lakehouse_or_sql_server  ''                                      Success (parent)\n",
    "        fn_save_in_single_file              fn_extract_lakehouse_or_sql_server      Danger (child)\n",
    "    \n",
    "Do we execute these 2 functions after \"silver\" (lh_bronze tables are recreated on every execution and we lose data if not in silver?\n",
    "For example: in broze we have 10 tables, but these 10 tables end up to 5 tables in silver. Consider the entire silver \"success\" as \"success\"?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4df47-4314-4075-8017-1ce915469d20",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Append to table \"lh_log.extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d720b07-8ac5-4d78-a10e-435396eef24f",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Append to table \"lh_log.extract\" the data for the successfully extracted tables\n",
    "# PL: Create lakehouse \"report\" for things that need to be reported?\n",
    "# PL: Skip this table as it can be generated from lh_log.log?\n",
    "if \"SQL Server\" in list_technology or \"Lakehouse\" in list_technology:\n",
    "    def fn_append_to_lh_log_extract():\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        if is_debug: par = {}\n",
    "\n",
    "        try:\n",
    "            # Create blank parameters, used to log \"Danger\"\n",
    "            rv_sql_code_locals = None\n",
    "            sdf_locals         = None\n",
    "            pdf_list           = None\n",
    "            rv_sdf             = None\n",
    "            sdf                = None\n",
    "\n",
    "            # Get the JSON column \"locals_\" for the successfully extracted table for the current \"process_timestamp\"\n",
    "            sql_code_locals = f\"\"\"SELECT `locals`\n",
    "FROM delta.`{global_parameter.abfs_path_lh_log}/Tables/log`\n",
    "WHERE\n",
    "  `medallion_name`        = \\\"Bronze\\\"\n",
    "  AND `source_name`       IN(\\\"fn_extract_lakehouse_or_sql_server\\\", \\\"fn_extract_excel\\\", \\\"fn_extract_csv\\\", \\\"fn_extract_json\\\")\n",
    "  AND `process_timestamp` = \\\"{global_parameter.process_timestamp}\\\" \n",
    "  AND `alert`             = \\\"Success\\\";\"\"\"\n",
    "            rv_sql_code_locals = fn_execute_spark_sql(sql_code_locals)\n",
    "            sdf_locals         = rv_sql_code_locals[2]\n",
    "            sdf_locals_count   = rv_sql_code_locals[3]\n",
    "\n",
    "            if sdf_locals_count > 0:\n",
    "                # Read JSON and concatenate to pipe separated list\n",
    "                sdf_locals = sdf_locals.withColumn(\"dict_parameter\", sf.get_json_object(sf.col(\"locals\"),\"$.dict_parameter\"))\n",
    "\n",
    "                sdf_locals = sdf_locals.select(\n",
    "                    sf.concat(\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"),\"$.technology\")    .alias(\"technology\")    , sf.lit(\"|\"),\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"),\"$.frequency\")     .alias(\"frequency\")     , sf.lit(\"|\"),\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"),\"$.lakehouse_name\").alias(\"lakehouse_name\"), sf.lit(\"|\"),\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"),\"$.server_alias\")  .alias(\"server_name\")   , sf.lit(\"|\"),\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"),\"$.database_name\") .alias(\"database_name\") , sf.lit(\"|\"),\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"),\"$.schema_name\")   .alias(\"schema_name\")   , sf.lit(\"|\"),\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"),\"$.table_name\")    .alias(\"table_name\")\n",
    "                    )\n",
    "                    .alias(\"I\")\n",
    "                )\n",
    "\n",
    "                pdf_list = sdf_locals.toPandas()\n",
    "                rv_list  = str(pdf_list['I'].tolist()).replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "                sql_code = f\"\"\"SELECT\n",
    "  T.`technology`\n",
    "  , T.`frequency`\n",
    "  , T.`lakehouse_name`\n",
    "  , T.`server_name`\n",
    "  , T.`database_name`\n",
    "  , T.`schema_name`\n",
    "  , T.`table_name`\n",
    "  , T.`folder_name`\n",
    "  , T.`file_name`\n",
    "  , T.`worksheet_name`\n",
    "  , MAX(CASE WHEN T.`A` = \\\"A\\\" THEN CAST(T.`value` AS TIMESTAMP) END) AS `extraction_date_start`\n",
    "  , MAX(CASE WHEN T.`A` = \\\"B\\\" THEN CAST(T.`value` AS TIMESTAMP) END) AS `extraction_date_end`\n",
    "  , \\\"{global_parameter.process_timestamp}\\\"                           AS `process_timestamp`\n",
    "FROM\n",
    "  (\n",
    "    /* Get rows for date_from */\n",
    "    SELECT\n",
    "      \\\"A\\\" AS `A`\n",
    "      , `technology`\n",
    "      , `frequency`\n",
    "      , `lakehouse_name`\n",
    "      , `server_name`\n",
    "      , `database_name`\n",
    "      , `schema_name`\n",
    "      , `table_name`\n",
    "      , \\\"\\\" AS `folder_name`\n",
    "      , \\\"\\\" AS `file_name`\n",
    "      , \\\"\\\" AS `worksheet_name`\n",
    "      , `value`\n",
    "    FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_parameter`\n",
    "    WHERE\n",
    "      CONCAT(\n",
    "        `technology`                    , \\\"|\\\"\n",
    "        , `frequency`                   , \\\"|\\\"\n",
    "        , IFNULL(`lakehouse_name`, \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(`server_name`   , \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(`database_name` , \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(`schema_name`   , \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(`table_name`    , \\\"\\\")\n",
    "      ) IN ({rv_list})\n",
    "      AND `name` = \\\"datetime_from\\\"\n",
    "\n",
    "    /* Get rows for date_to */\n",
    "    UNION SELECT\n",
    "      \\\"B\\\" AS `A`\n",
    "      , `technology`\n",
    "      , `frequency`\n",
    "      , `lakehouse_name`\n",
    "      , `server_name`\n",
    "      , `database_name`\n",
    "      , `schema_name`\n",
    "      , `table_name`\n",
    "      , \\\"\\\" AS `folder_name`\n",
    "      , \\\"\\\" AS `file_name`\n",
    "      , \\\"\\\" AS `worksheet_name`\n",
    "      , `value`\n",
    "    FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_parameter`\n",
    "    WHERE\n",
    "      CONCAT(\n",
    "        `technology`                    , \\\"|\\\"\n",
    "        , `frequency`                   , \\\"|\\\"\n",
    "        , IFNULL(`lakehouse_name`, \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(`server_name`   , \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(`database_name` , \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(`schema_name`   , \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(`table_name`    , \\\"\\\")\n",
    "      ) IN ({rv_list})\n",
    "      AND `name` = \\\"datetime_to\\\"\n",
    "\n",
    "    /* Get rows for not filtered tables */\n",
    "    UNION SELECT\n",
    "      \\\"C\\\" AS A\n",
    "      , C.`technology`\n",
    "      , C.`frequency`\n",
    "      , C.`lakehouse_name`\n",
    "      , C.`server_name`\n",
    "      , C.`database_name`\n",
    "      , C.`schema_name`\n",
    "      , C.`table_name`\n",
    "      , C.`folder_name`\n",
    "      , C.`file_name`\n",
    "      , C.`worksheet_name`\n",
    "      , NULL AS `value`\n",
    "    FROM\n",
    "      (\n",
    "        SELECT\n",
    "          `technology`\n",
    "          , `frequency`\n",
    "          , \\\"\\\" AS `lakehouse_name`\n",
    "          , `server_name`\n",
    "          , `database_name`\n",
    "          , `schema_name`\n",
    "          , `table_name`\n",
    "          , \\\"\\\" AS `folder_name`\n",
    "          , \\\"\\\" AS `file_name`\n",
    "          , \\\"\\\" AS `worksheet_name`\n",
    "          , `is_extracted`\n",
    "        FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_sql_server`\n",
    "\n",
    "        UNION SELECT\n",
    "          `technology`\n",
    "          , `frequency`\n",
    "          , `lakehouse_name`\n",
    "          , \\\"\\\" AS `server_name`\n",
    "          , \\\"\\\" AS `database_name`\n",
    "          , \\\"\\\" AS `schema_name`\n",
    "          , `table_name`\n",
    "          , \\\"\\\" AS `folder_name`\n",
    "          , \\\"\\\" AS `file_name`\n",
    "          , \\\"\\\" AS `worksheet_name`\n",
    "          , `is_extracted`\n",
    "        FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_lakehouse`\n",
    "\n",
    "        UNION SELECT\n",
    "          `technology`\n",
    "          , `frequency`\n",
    "          , \\\"\\\" AS `lakehouse_name`\n",
    "          , \\\"\\\" AS `server_name`\n",
    "          , \\\"\\\" AS `database_name`\n",
    "          , \\\"\\\" AS `schema_name`\n",
    "          , \\\"\\\" AS `table_name`\n",
    "          , `folder_name`\n",
    "          , `file_name`\n",
    "          , `worksheet_name`\n",
    "          , `is_extracted`\n",
    "        FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_excel`\n",
    "\n",
    "        UNION SELECT\n",
    "          `technology`\n",
    "          , `frequency`\n",
    "          , \\\"\\\" AS `lakehouse_name`\n",
    "          , \\\"\\\" AS `server_name`\n",
    "          , \\\"\\\" AS `database_name`\n",
    "          , \\\"\\\" AS `schema_name`\n",
    "          , \\\"\\\" AS `table_name`\n",
    "          , `folder_name`\n",
    "          , `file_name`\n",
    "          , \\\"\\\" AS `worksheet_name`\n",
    "          , `is_extracted`\n",
    "        FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_csv`\n",
    "\n",
    "        UNION SELECT\n",
    "          `technology`\n",
    "          , `frequency`\n",
    "          , \\\"\\\" AS `lakehouse_name`\n",
    "          , \\\"\\\" AS `server_name`\n",
    "          , \\\"\\\" AS `database_name`\n",
    "          , \\\"\\\" AS `schema_name`\n",
    "          , \\\"\\\" AS `table_name`\n",
    "          , `folder_name`\n",
    "          , `file_name`\n",
    "          , \\\"\\\" AS `worksheet_name`\n",
    "          , `is_extracted`\n",
    "        FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_json`\n",
    "      ) AS C\n",
    "    WHERE\n",
    "      CONCAT(\n",
    "        C.`technology`                    , \\\"|\\\"\n",
    "        , C.`frequency`                   , \\\"|\\\"\n",
    "        , IFNULL(C.`lakehouse_name`, \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(C.`server_name`   , \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(C.`database_name` , \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(C.`schema_name`   , \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(C.`table_name`    , \\\"\\\")\n",
    "      ) IN ({rv_list})\n",
    "      AND C.`is_extracted` = 1\n",
    "  ) AS T\n",
    "GROUP BY\n",
    "  T.`technology`\n",
    "  , T.`frequency`\n",
    "  , T.`lakehouse_name`\n",
    "  , T.`server_name`\n",
    "  , T.`database_name`\n",
    "  , T.`schema_name`\n",
    "  , T.`table_name`\n",
    "  , T.`folder_name`\n",
    "  , T.`file_name`\n",
    "  , T.`worksheet_name`\n",
    "ORDER BY\n",
    "  T.`technology`\n",
    "  , T.`frequency`\n",
    "  , T.`lakehouse_name`\n",
    "  , T.`server_name`\n",
    "  , T.`database_name`\n",
    "  , T.`schema_name`\n",
    "  , T.`table_name`\n",
    "  , T.`folder_name`\n",
    "  , T.`file_name`\n",
    "  , T.`worksheet_name`;\"\"\"\n",
    "                rv_sdf    = fn_execute_spark_sql(sql_code)\n",
    "                sdf       = rv_sdf[2]\n",
    "                sdf_count = rv_sdf[3]\n",
    "\n",
    "                sdf.write.format(\"delta\")\\\n",
    "                    .mode(\"append\")\\\n",
    "                    .save(f\"{global_parameter.abfs_path_lh_log}/Tables/extract\")\n",
    "\n",
    "            if \"sdf_count\" not in locals(): sdf_count = 0\n",
    "\n",
    "            alert             = \"Success\"\n",
    "            alert_description = f\"Inserted rows (count): {sdf_count}\"\n",
    "\n",
    "            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n",
    "\n",
    "            return (alert, alert_description)\n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            return (alert, alert_description)\n",
    "        finally:\n",
    "          if is_debug:\n",
    "            par[\"locals\"] = locals()\n",
    "            if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n",
    "            fn_print_debug_info(alert, fn_name, par)\n",
    "            del par\n",
    "\n",
    "          fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853654eb-6bc2-41c4-868f-2ada0924252f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Update values 'datetime_from' and \"datetime_to\" in table \"lh_cfg.extract_parameter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70af2d50-51f3-4dd7-ad26-017a8dc53b3e",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "# Update (after the execution of the extraction loop):\n",
    "#   • \"datetime_from\" with \"datatime_to\"\n",
    "#   • \"datetime_to\" with NULL\n",
    "# for the successfully extracted tables\n",
    "if \"SQL Server\" in list_technology or \"Lakehouse\" in list_technology:\n",
    "    def fn_update_datetime_from_datetime_to_in_extract_parameter(pdf_log):\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        if is_debug: par = {}\n",
    "\n",
    "        try:\n",
    "            # Get the JSON column \"locals_\" for the successfully extracted tables\n",
    "            pdf_locals = pdf_log.loc[\n",
    "                (pdf_log[\"medallion_name\"] == \"Bronze\")\n",
    "                & (pdf_log[\"source_name\"] == \"fn_extract_lakehouse_or_sql_server\")\n",
    "                & (pdf_log[\"alert\"] == \"Success\")\n",
    "            , [\"locals\"]]\n",
    "            pdf_locals_count = len(pdf_locals.index)        \n",
    "\n",
    "            if pdf_locals_count > 0:\n",
    "                # Create spark dataframe\n",
    "                sdf_locals = spark.createDataFrame(pdf_locals)\n",
    "                sdf_locals_count = sdf_locals.count()\n",
    "\n",
    "                # Read JSON and concatenate to \"pipe separated list\"\n",
    "                sdf_locals = sdf_locals.withColumn(\"dict_parameter\", sf.get_json_object(sf.col(\"locals\"),\"$.dict_parameter\"))\n",
    "                sdf_locals = sdf_locals.select(\n",
    "                    sf.concat(\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"), \"$.technology\")    .alias(\"technology\")    , sf.lit(\"|\"),\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"), \"$.frequency\")     .alias(\"frequency\")     , sf.lit(\"|\"),\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"), \"$.lakehouse_name\").alias(\"lakehouse_name\"), sf.lit(\"|\"),\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"), \"$.server_alias\")  .alias(\"server_name\")   , sf.lit(\"|\"),\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"), \"$.database_name\") .alias(\"database_name\") , sf.lit(\"|\"),\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"), \"$.schema_name\")   .alias(\"schema_name\")   , sf.lit(\"|\"),\n",
    "                        sf.get_json_object(sf.col(\"dict_parameter\"), \"$.table_name\")    .alias(\"table_name\")\n",
    "                    )\n",
    "                    .alias(\"I\")\n",
    "                )\n",
    "\n",
    "                pdf_list = sdf_locals.toPandas()\n",
    "                rv_list  = str(pdf_list['I'].tolist()).replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "                sql_code = f\"\"\"SELECT\n",
    "    `technology`\n",
    "    , `frequency`\n",
    "    , `lakehouse_name`\n",
    "    , `server_name`\n",
    "    , `database_name`\n",
    "    , `schema_name`\n",
    "    , `table_name`\n",
    "    , `value`\n",
    "    , CAST(NULL AS VARCHAR(8000)) AS `N`\n",
    "FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_parameter`\n",
    "WHERE\n",
    "    CONCAT(\n",
    "        `technology`              , \\\"|\\\"\n",
    "        , `frequency`             , \\\"|\\\"\n",
    "        , IFNULL(`lakehouse_name`, \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(`server_name`   , \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(`database_name` , \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(`schema_name`   , \\\"\\\"), \\\"|\\\"\n",
    "        , IFNULL(`table_name`    , \\\"\\\")\n",
    "    ) IN ({rv_list})\n",
    "    AND name = \\\"datetime_to\\\";\"\"\"\n",
    "                sdf_src = fn_execute_spark_sql(sql_code)[2]\n",
    "                \n",
    "                # Get the destination table (lh_cfg.extract_parameter)\n",
    "                dstn = dlt.DeltaTable.forPath(spark, f\"{global_parameter.abfs_path_lh_cfg}/Tables/extract_parameter\")\n",
    "\n",
    "                # Update \"datetime_from\" with \"datatime_to\" for the successfully extracted tables - End #\n",
    "                dstn.alias(\"dstn\") \\\n",
    "                .merge(\n",
    "                    sdf_src.alias(\"src\"),\n",
    "                    \"coalesce(dstn.technology       , \\\"\\\") = coalesce(src.technology    , \\\"\\\") \\\n",
    "                    and coalesce(dstn.frequency     , \\\"\\\") = coalesce(src.frequency     , \\\"\\\") \\\n",
    "                    and coalesce(dstn.lakehouse_name, \\\"\\\") = coalesce(src.lakehouse_name, \\\"\\\") \\\n",
    "                    and coalesce(dstn.server_name   , \\\"\\\") = coalesce(src.server_name   , \\\"\\\") \\\n",
    "                    and coalesce(dstn.database_name , \\\"\\\") = coalesce(src.database_name , \\\"\\\") \\\n",
    "                    and coalesce(dstn.schema_name   , \\\"\\\") = coalesce(src.schema_name   , \\\"\\\") \\\n",
    "                    and coalesce(dstn.table_name    , \\\"\\\") = coalesce(src.table_name    , \\\"\\\") \\\n",
    "                    and dstn.name                           = 'datetime_from'\"\n",
    "                ) \\\n",
    "                .whenMatchedUpdate(set = {\"value\": \"src.value\"}) \\\n",
    "                .execute()\n",
    "                # Update \"datetime_from\" with \"datatime_to\" for the successfully extracted tables - End#\n",
    "\n",
    "                # Update \"datetime_to\" with NULL for the successfully extracted tables - Start # \n",
    "                dstn.alias(\"dstn\") \\\n",
    "                .merge(\n",
    "                    sdf_src.alias(\"src\"),\n",
    "                    \"coalesce(dstn.technology       , \\\"\\\") = coalesce(src.technology    , \\\"\\\") \\\n",
    "                    and coalesce(dstn.frequency     , \\\"\\\") = coalesce(src.frequency     , \\\"\\\") \\\n",
    "                    and coalesce(dstn.lakehouse_name, \\\"\\\") = coalesce(src.lakehouse_name, \\\"\\\") \\\n",
    "                    and coalesce(dstn.server_name   , \\\"\\\") = coalesce(src.server_name   , \\\"\\\") \\\n",
    "                    and coalesce(dstn.database_name , \\\"\\\") = coalesce(src.database_name , \\\"\\\") \\\n",
    "                    and coalesce(dstn.schema_name   , \\\"\\\") = coalesce(src.schema_name   , \\\"\\\") \\\n",
    "                    and coalesce(dstn.table_name    , \\\"\\\") = coalesce(src.table_name    , \\\"\\\") \\\n",
    "                    and dstn.name                           = 'datetime_to'\"\n",
    "                ) \\\n",
    "                .whenMatchedUpdate(set = {\"value\": \"src.N\"}) \\\n",
    "                .execute()\n",
    "                # Update \"datetime_to\" with NULL for the successfully extracted tables - End #                \n",
    "         \n",
    "            alert             = \"Success\"\n",
    "            alert_description = \"\"\n",
    "\n",
    "            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n",
    "            \n",
    "            return (alert, alert_description)\n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            return (alert, alert_description)\n",
    "        finally:\n",
    "            if is_debug:\n",
    "                par[\"locals\"] = locals()\n",
    "                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n",
    "                fn_print_debug_info(alert, fn_name, par)\n",
    "                del par\n",
    "\n",
    "            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e1562-8980-42dc-88e7-b189e3e6c242",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7896b7a-5b13-42e1-bcba-a3049e53da1f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Extract technology \"Lakehouse\", \"SQL Server\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a4929-e615-4745-9b75-2f7d42bccbe9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if \"SQL Server\" in list_technology or \"Lakehouse\" in list_technology:\n",
    "    def fn_extract_lakehouse_or_sql_server(technology, frequency, dict_server_alias_server_name):\n",
    "        fn_name        = stk()[0][3]\n",
    "        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n",
    "        #if is_debug: par = {}\n",
    "\n",
    "        try:\n",
    "            now_year  = fn_get_now(\"year\")[1]\n",
    "            now_month = fn_get_now(\"month\")[1]\n",
    "\n",
    "            # Add initial parameters in table \"lh_cfg.extract_parameter\"\n",
    "            fn_add_initial_extract_parameter(technology, frequency)\n",
    "\n",
    "            # Add now() as datetime_to in table \"lh_cfg.extract_parameter\" for all the tables\n",
    "            fn_update_datetime_to_in_extract_parameter(technology, frequency)\n",
    "\n",
    "            # Get extract table list\n",
    "            dict_parameter_table_list     = {\"technology\": technology, \"frequency\": frequency, \"action\": \"Extract\"}\n",
    "            rv_extract_table_list         = fn_get_extract_table_list(dict_parameter_table_list)\n",
    "            sdf_extract_table_list_status = rv_extract_table_list[0]\n",
    "            sdf_extract_table_list        = rv_extract_table_list[2]\n",
    "            sdf_extract_table_list_count  = rv_extract_table_list[3]\n",
    "            if is_debug:\n",
    "                print(f\"dict_parameter_table_list: {dict_parameter_table_list}\")\n",
    "                print(f\"rv_extract_table_list: {rv_extract_table_list}\")\n",
    "                print(f\"sdf_extract_table_list_status: {sdf_extract_table_list_status}\")\n",
    "                print(f\"sdf_extract_table_list:\")\n",
    "                display(sdf_extract_table_list)\n",
    "                print(f\"sdf_extract_table_list_count:  {sdf_extract_table_list_count}\")\n",
    "\n",
    "            # Loop extract table list\n",
    "            if sdf_extract_table_list_count > 0:\n",
    "                # etl_r --> sdf_extract_table_list.row\n",
    "                for etl_r in sdf_extract_table_list \\\n",
    "                    .sort(sdf_extract_table_list.technology.asc(),\n",
    "                        sdf_extract_table_list.frequency.asc(),\n",
    "                        sdf_extract_table_list.lakehouse_name.asc(),\n",
    "                        sdf_extract_table_list.server_name.asc(),\n",
    "                        sdf_extract_table_list.database_name.asc(),\n",
    "                        sdf_extract_table_list.schema_name.asc(),\n",
    "                        sdf_extract_table_list.table_name.asc()\n",
    "                    ).collect():\n",
    "\n",
    "                    if technology == \"SQL Server\":\n",
    "                        server_alias = etl_r.server_name\n",
    "                        server_name  = fn_get_server_name_for_server_alias(dict_server_alias_server_name, server_alias)[2]\n",
    "\n",
    "                    if technology == \"Lakehouse\":\n",
    "                        server_alias = etl_r.server_name\n",
    "                        server_name  = etl_r.server_name\n",
    "                    \n",
    "                    if is_debug:\n",
    "                        print(f\"server_alias: {type(server_alias)} | {server_alias}\")\n",
    "                        print(f\"server_name: {type(server_name)} | {server_name}\")                        \n",
    "                        print(f\"sdf_extract_table_list.etl_r:\")  \n",
    "                        print(f\"  technology:                    {etl_r.technology}\")\n",
    "                        print(f\"  frequency:                     {etl_r.frequency}\")\n",
    "                        print(f\"  lakehouse_name:                {etl_r.lakehouse_name}\")\n",
    "                        print(f\"  server_name:                   {etl_r.server_name}\")\n",
    "                        print(f\"  database_name:                 {etl_r.database_name}\")\n",
    "                        print(f\"  schema_name:                   {etl_r.schema_name}\")\n",
    "                        print(f\"  table_name:                    {etl_r.table_name}\")\n",
    "                        print(f\"  keyvault_url:                  {etl_r.keyvault_url}\")\n",
    "                        print(f\"  keyvault_secret_name_user:     {etl_r.keyvault_secret_name_user}\")\n",
    "                        print(f\"  keyvault_secret_name_password: {etl_r.keyvault_secret_name_password}\")\n",
    "\n",
    "                    dict_parameter = {\"process_timestamp\": global_parameter.process_timestamp\n",
    ", \"technology\": etl_r.technology\n",
    ", \"frequency\": etl_r.frequency\n",
    ", \"lakehouse_name\": etl_r.lakehouse_name\n",
    ", \"server_alias\": server_alias\n",
    ", \"server_name\": server_name\n",
    ", \"database_name\": etl_r.database_name\n",
    ", \"schema_name\": etl_r.schema_name\n",
    ", \"table_name\": etl_r.table_name\n",
    ", \"keyvault_url\": etl_r.keyvault_url\n",
    ", \"keyvault_secret_name_user\": etl_r.keyvault_secret_name_user\n",
    ", \"keyvault_secret_name_password\": etl_r.keyvault_secret_name_password}\n",
    "\n",
    "                    if is_debug: print(f\"dict_parameter: {type(dict_parameter)} | {dict_parameter}\")\n",
    "\n",
    "                    # Generate column list\n",
    "                    rv_extract_column         = fn_get_extract_column(dict_parameter)\n",
    "                    sdf_extract_column_status = rv_extract_column[0]\n",
    "                    sdf_extract_column        = rv_extract_column[2]\n",
    "                    sdf_extract_column_count  = rv_extract_column[3]\n",
    "                    if is_debug:\n",
    "                        print(f\"sdf_extract_column_status: {sdf_extract_column_status}\")\n",
    "                        print(f\"sdf_extract_column:\")\n",
    "                        display(sdf_extract_column)\n",
    "                        print(f\"sdf_extract_column_count: {sdf_extract_column_count}\")\n",
    "\n",
    "                    # Generate dynamic SELECT statement\n",
    "                    rv_stmt_select     = fn_get_statament_select(sdf_extract_column, etl_r.technology)\n",
    "                    stmt_select_status = rv_stmt_select[0]\n",
    "                    stmt_select        = rv_stmt_select[2]\n",
    "                    if is_debug:\n",
    "                        print(f\"stmt_select_status: {stmt_select_status}\")\n",
    "                        print(f\"stmt_select:        {stmt_select}\")\n",
    "\n",
    "                    # Generate dynamic FROM statement\n",
    "                    rv_stmt_from     = fn_get_statament_from(sdf_extract_column, etl_r.lakehouse_name, etl_r.schema_name, etl_r.table_name, technology)\n",
    "                    stmt_from_status = rv_stmt_from[0]\n",
    "                    stmt_from        = rv_stmt_from[2]\n",
    "                    if is_debug:\n",
    "                        print(f\"stmt_from_status: {stmt_from_status}\")\n",
    "                        print(f\"stmt_from: {stmt_from}\")\n",
    "\n",
    "                    # Generate dynamic WHERE statement\n",
    "                    rv_stmt_where     = fn_get_statement_where(sdf_extract_column, dict_parameter)\n",
    "                    stmt_where_status = rv_stmt_where[0]\n",
    "                    stmt_where        = rv_stmt_where[2]\n",
    "                    if technology == \"Lakehouse\":\n",
    "                        stmt_where = stmt_where.replace(\"[\", \"`\").replace(\"]\", \"`\")\n",
    "                    if is_debug:\n",
    "                        print(f\"stmt_where_status: {stmt_where_status}\")\n",
    "                        print(f\"stmt_where: {stmt_where}\")\n",
    "\n",
    "                    # Generate SELECT FROM WHERE statement\n",
    "                    stmt_extract = f\"{stmt_select} {stmt_from} {stmt_where}\"\n",
    "                    dict_parameter[\"sql_query\"] = stmt_extract\n",
    "                    if is_debug:\n",
    "                        print(f\"stmt_extract: {stmt_extract}\")\n",
    "                        print(f\"dict_parameter: {dict_parameter}\")\n",
    "\n",
    "                    # Extract to SDF - Start\n",
    "                    # On-premises SQL Server\n",
    "                    if technology == \"SQL Server\":\n",
    "                        if is_debug: print(\"Extract from on-premises SQL Server\")\n",
    "                        rv_execute_sql_query = fn_execute_sql_query(dict_parameter)\n",
    "\n",
    "                    # Lakehouse\n",
    "                    if technology == \"Lakehouse\":\n",
    "                        if is_debug: \n",
    "                            print(\"technology = \\\"Lakehouse\\\"\")\n",
    "                            print(f\"dict_parameter: {dict_parameter}\")\n",
    "                        rv_execute_sql_query = fn_execute_spark_sql(stmt_extract)\n",
    "                    \n",
    "                    sdf_extracted_status = rv_execute_sql_query[0]\n",
    "                    sdf_extracted        = rv_execute_sql_query[2]\n",
    "                    sdf_extracted_count  = rv_execute_sql_query[3]\n",
    "\n",
    "                    if is_debug:\n",
    "                        print(f\"rv_execute_sql_query: {rv_execute_sql_query}\")\n",
    "                        print(f\"sdf_extracted_status: {sdf_extracted_status}\")\n",
    "                        print(f\"sdf_extracted (schema):\")\n",
    "                        sdf_extracted.printSchema()\n",
    "                        print(f\"sdf_extracted:\")\n",
    "                        display(sdf_extracted)\n",
    "                        print(f\"sdf_extracted_count: {sdf_extracted_count}\")\n",
    "                    # Extract to SDF - End\n",
    "\n",
    "                    if sdf_extracted_status == \"Success\":\n",
    "                        # Save extracted dataframe as table in \"lh_bronze\" - Start\n",
    "                        lakehouse_name_clean = fn_clean_accent_in_string(etl_r.lakehouse_name)[2]\n",
    "                        server_name_clean    = fn_clean_accent_in_string(etl_r.server_name)[2]\n",
    "                        database_name_clean  = fn_clean_accent_in_string(etl_r.database_name)[2]\n",
    "                        schema_name_clean    = fn_clean_accent_in_string(etl_r.schema_name)[2]\n",
    "                        table_name_clean     = fn_clean_accent_in_string(etl_r.table_name)[2]\n",
    "\n",
    "                        lakehouse_name_clean = fn_clean_not_alphanumeric_in_string(lakehouse_name_clean, \"_\")[2]\n",
    "                        server_name_clean    = fn_clean_not_alphanumeric_in_string(server_name_clean, \"_\")[2]\n",
    "                        database_name_clean  = fn_clean_not_alphanumeric_in_string(database_name_clean, \"_\")[2]\n",
    "                        schema_name_clean    = fn_clean_not_alphanumeric_in_string(schema_name_clean, \"_\")[2]\n",
    "                        table_name_clean     = fn_clean_not_alphanumeric_in_string(table_name_clean, \"_\")[2]\n",
    "\n",
    "                        dict_parameter[\"lakehouse_name_clean\"] = lakehouse_name_clean\n",
    "                        dict_parameter[\"server_name_clean\"] = server_name_clean\n",
    "                        dict_parameter[\"database_name_clean\"] = database_name_clean\n",
    "                        dict_parameter[\"schema_name_clean\"] = schema_name_clean\n",
    "                        dict_parameter[\"table_name_clean\"] = table_name_clean\n",
    "\n",
    "                        if technology == \"SQL Server\": sdf_extracted_table_path = f\"{global_parameter.abfs_path_lh_bronze}/Tables/{server_name_clean}\\\n",
    "_{database_name_clean}_{schema_name_clean}_{table_name_clean}\"\n",
    "\n",
    "                        if technology == \"Lakehouse\": sdf_extracted_table_path = f\"{global_parameter.abfs_path_lh_bronze}/Tables/{lakehouse_name_clean}\\\n",
    "_{table_name_clean}\"\n",
    "                        fn_save_sdf_as_table(\n",
    "                            sdf_extracted\n",
    "                            , \"delta\"\n",
    "                            , \"overwrite\"\n",
    "                            , sdf_extracted_table_path\n",
    "                            , dict_parameter\n",
    "                        )\n",
    "\n",
    "                        if is_debug:\n",
    "                            print(\"sdf_extracted - Start\")\n",
    "                            print(f\"sdf_extracted_table_path: {sdf_extracted_table_path}\")\n",
    "                            sdf_extracted.printSchema()\n",
    "                            display(sdf_extracted)\n",
    "                            print(\"sdf_extracted - End\")\n",
    "                        # Save extracted dataframe as table in \"lh_bronze\" - End\n",
    "\n",
    "                        # Save extracted dataframe as single .parquet file in \"lh_bronze/Processed/<technology>\" (archive) - Start\n",
    "                        if technology == \"SQL Server\": sdf_extracted_archive_path = f\"{global_parameter.abfs_path_lh_bronze}/Files/processed/{technology}/\\\n",
    "{server_name_clean}/{database_name_clean}/{schema_name_clean}/{table_name_clean}/\\\n",
    "{now_year}/{now_month}/\\\n",
    "{table_name_clean}_{global_parameter.process_timestamp}.parquet\"\n",
    "\n",
    "                        if technology == \"Lakehouse\": sdf_extracted_archive_path = f\"{global_parameter.abfs_path_lh_bronze}/Files/processed/{technology}/\\\n",
    "{lakehouse_name_clean}/{table_name_clean}/\\\n",
    "{now_year}/{now_month}/\\\n",
    "{table_name_clean}_{global_parameter.process_timestamp}.parquet\"\n",
    "\n",
    "                        rv_save_in_single_file = fn_save_in_single_file(sdf_extracted, sdf_extracted_archive_path)\n",
    "                        if is_debug:\n",
    "                            print(f\"alert: {rv_save_in_single_file[0]}\")\n",
    "                            print(f\"alert_description: {rv_save_in_single_file[1]}\")\n",
    "                        # Save extracted dataframe as single .parquet file in \"lh_bronze/Processed/<technology>\" (archive) - End\n",
    "\n",
    "                    # Insert \"Success\" in lh_log (for each table) - Start\n",
    "                    alert             = \"Success\"\n",
    "                    alert_description = f\"Row count: {str(sdf_extracted_count)}\"\n",
    "\n",
    "                    if is_debug:\n",
    "                        par = {}\n",
    "                        par[\"locals\"] = locals()\n",
    "                        fn_print_debug_info(alert, fn_name, par)\n",
    "                        del par\n",
    "                    \n",
    "                    fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)\n",
    "                    # Insert \"Success\" in lh_log (for each table) - End\n",
    "\n",
    "                    if is_debug: print(f\"\\n------------------------------ Loop - End ------------------------------\\n\")\n",
    "\n",
    "            return (alert, alert_description)\n",
    "        except Exception as ex:\n",
    "            alert             = \"Danger\"\n",
    "            alert_description = str(ex)\n",
    "\n",
    "            if is_debug:\n",
    "                par = {}\n",
    "                par[\"locals\"] = locals()\n",
    "                fn_print_debug_info(alert, fn_name, par)\n",
    "                del par\n",
    "\n",
    "            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)\n",
    "\n",
    "            return (alert, alert_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a8f15-b420-4045-aa19-7cbfae92ad19",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if is_debug: print(\"\\n---------- This notenook name: nb_bronze_function_extract_lakehouse_sql_server - End ----------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
