{"cells":[{"cell_type":"markdown","source":["# System"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"acf4724c-69e0-4cfd-88d6-690e912ef325"},{"cell_type":"code","source":["if is_debug: print(\"\\n---------- This notenook name: nb_bronze_function_extract_json - Start ----------\\n\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d4e5c656-d839-409c-b258-7bdaa0968fc3"},{"cell_type":"code","source":["medallion_name = \"Bronze\"\n","if is_debug: print(f\"medallion_name: {medallion_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"13c2428b-9d8c-4cb3-bd1c-129620a77276"},{"cell_type":"code","source":["# Current session info\n","if is_debug: display(ss.getActiveSession())"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"30af8da7-0981-4161-b984-4bac3b3fcca2"},{"cell_type":"markdown","source":["# Functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1446df47-3b52-4585-a4b1-7228929ad3d8"},{"cell_type":"markdown","source":["## Create temp view cfg"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dd502214-54ed-4442-8bf7-747b56e93ab3"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_create_tvw_cfg(dict_parameter):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            sql_code = f\"\"\"SELECT\n","    `sequence`\n","    , `column_name`\n","    , `parent_column_name`\n","    , `data_type`\n","FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_json`\n","WHERE\n","    `technology`                  = \\\"{dict_parameter[\"technology\"]}\\\"\n","    AND `frequency`               = \\\"{dict_parameter[\"frequency\"]}\\\"\n","    AND IFNULL(`folder_name`, \"\") = \\\"{dict_parameter[\"folder_name\"]}\\\"\n","    AND IFNULL(`file_name`, \"\")   = \\\"{dict_parameter[\"file_name\"]}\\\"\n","    AND `is_extracted`            = 1\n","ORDER BY `sequence`;\"\"\"\n","\n","            rv        = fn_execute_spark_sql(sql_code)\n","            sdf       = rv[2]\n","            sdf_count = rv[3]\n","\n","            sdf.createOrReplaceTempView('tvw_cfg')\n","\n","            alert             = \"Success\"\n","            alert_description = f\"tvw_cfg (count): {sdf_count}\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, sdf, sdf_count)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false}},"id":"026136d7-2cf0-45b3-bea8-f2115770655c"},{"cell_type":"markdown","source":["## Get root array info"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1def0350-4c20-4999-b6dd-936c340294ee"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_get_root_array_info():\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            sql_code = f\"\"\"SELECT column_name\n","    FROM tvw_cfg\n","    WHERE\n","        parent_column_name = \\\"root\\\"\n","        AND data_type = \\\"Array\\\"\n","            \"\"\"\n","            rv               = fn_execute_spark_sql(sql_code)\n","            sdf_root_array   = rv[2]\n","            root_array_count = rv[3]\n","\n","            if root_array_count > 0:\n","                does_root_array_exist  = True\n","                root_array_column_name = sdf_root_array.collect()[0][0]\n","            else:\n","                does_root_array_exist  = False\n","                root_array_column_name = \"\"\n","                \n","            alert             = \"Success\"\n","            alert_description = f\"Root array (exists?, name): {does_root_array_exist}, \\\"{root_array_column_name}\\\"\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, does_root_array_exist, root_array_column_name)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"31bfcbeb-0a3c-4fec-bc04-43c208787959"},{"cell_type":"markdown","source":["## Create temp view parent_child"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e398f6f6-69f2-4645-bdb7-2235f55b01d7"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_create_cfg_parent_child(does_root_array_exist, root_array_column_name):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            level = 1\n","\n","            # Create root array row\n","            if does_root_array_exist == True:\n","                sql_code = f\"\"\"SELECT\n","    `sequence`\n","    , 1                                  AS `level`\n","    , `column_name`\n","    , IFNULL(`parent_column_name`, \\\"\\\") AS `parent_column_name`\n","    , ''                                 AS `breadcrumb_column_name`\n","    , ''                                 AS `breadcrumb_parent_column_name`\n","    , `data_type`\n","FROM `tvw_cfg`\n","WHERE\n","    `data_type`              = \\\"Array\\\"\n","    AND `parent_column_name` = \\\"root\\\";\"\"\"\n","            else:\n","                sql_code = f\"\"\"    SELECT\n","    0         AS `sequence`\n","    , 0       AS `level`\n","    , ''      AS `column_name`\n","    , 'root'  AS `parent_column_name`\n","    , ''      AS `breadcrumb_column_name`\n","    , ''      AS `breadcrumb_parent_column_name`\n","    , 'Array' AS `data_type`;\"\"\"\n","            \n","            sdf = fn_execute_spark_sql(sql_code)[2]\n","            sdf.createOrReplaceTempView('tvw_recursive')\n","\n","            # Loop recursively to create levels\n","            while True:\n","                sql_code = f\"\"\"SELECT\n","    P.`sequence`\n","    , {level} + 1 AS `level`\n","    , IFNULL(P.`column_name`       , \\\"\\\") AS`column_name`\n","    , IFNULL(P.`parent_column_name`, \\\"\\\") AS `parent_column_name`        \n","    , CONCAT(\n","        C.`breadcrumb_column_name`\n","        , IF(C.`breadcrumb_column_name` = \\\"\\\", \\\"\\\", \\\"_\\\")\n","        , P.`column_name`\n","    ) AS `breadcrumb_column_name`\n","    , IF(\n","        IFNULL(P.`parent_column_name`, \\\"\\\") = \\\"{root_array_column_name}\\\"\n","        , \\\"\\\"\n","        , CONCAT(\n","            C.`breadcrumb_parent_column_name`\n","            , IF(C.`breadcrumb_parent_column_name` = \\\"\\\", \\\"\\\", \\\"_\\\")\n","            , P.`parent_column_name`\n","        )\n","    ) AS `breadcrumb_parent_column_name`\n","    , IFNULL(P.`data_type`         , \\\"\\\") AS `data_type`\n","FROM\n","`tvw_cfg`            AS P\n","JOIN `tvw_recursive` AS C ON IFNULL(P.`parent_column_name`, \\\"\\\") = C.`column_name`;\"\"\"\n","\n","                sdf_recursive = fn_execute_spark_sql(sql_code)[2]\n","                sdf_recursive.createOrReplaceTempView('tvw_recursive')\n","                sdf = sdf.union(sdf_recursive)\n","\n","                if sdf_recursive.count() == 0:\n","                    sdf.createOrReplaceTempView(\"tvw_final\")\n","                    break\n","                else:\n","                    level += 1\n","\n","            # Add column \"child_list\" and fake root array row\n","            sql_code = f\"\"\"CREATE OR REPLACE TEMP VIEW tvw_cfg_parent_child AS\n","\"\"\"\n","\n","            if does_root_array_exist == False:\n","                sql_code += f\"\"\"SELECT\n","    CAST(0 AS Long) AS `sequence`\n","    , 1             AS `level`\n","    , 'results'     AS `column_name`\n","    , 'root'        AS `parent_column_name`\n","    , ''            AS `breadcrumb_column_name`\n","    , ''            AS `breadcrumb_parent_column_name`\n","    , 'Array'       AS `data_type`\n","    , CAST(collect_list(`column_name`) AS String) AS `child_list`\n","FROM `tvw_final` AS H\n","WHERE\n","    H.`data_type` IN ('Array', 'Struct')\n","    AND H.`parent_column_name` = ''\n","UNION ALL \"\"\"\n","\n","            sql_code += f\"\"\"SELECT\n","    F.`sequence`\n","    , F.`level`\n","    , F.`column_name`\n","    , IF(F.`parent_column_name` = '', 'results', F.`parent_column_name`) AS `parent_column_name`\n","    , F.`breadcrumb_column_name`\n","    , F.`breadcrumb_parent_column_name`\n","    , F.`data_type`\n","    , IFNULL(G.`child_list`, '') AS `child_list`\n","FROM\n","    `tvw_final` AS F\n","    LEFT JOIN \n","    (\n","        SELECT\n","            `parent_column_name`\n","            , CAST(collect_list(`column_name`) AS String) as `child_list`\n","        FROM `tvw_cfg`\n","        WHERE\n","            `parent_column_name` != 'root'\n","            AND `data_type` in ('Array', 'Struct')\n","        GROUP BY `parent_column_name`\n","    ) AS G\n","    ON F.`column_name` = G.`parent_column_name`\n","WHERE F.`sequence` > 0\n","ORDER BY\n","    `level`\n","    , `sequence`;\"\"\"\n","\n","            fn_execute_spark_sql(sql_code)[2]\n","            sdf_cfg = fn_execute_spark_sql(\"SELECT * FROM `tvw_cfg_parent_child`;\")[2]\n","            sdf_cfg.createOrReplaceTempView(\"tvw_cfg\")\n","            sdf_cfg_count = sdf_cfg.count()\n","\n","            alert             = \"Success\"\n","            alert_description = f\"Rows (count): {sdf_cfg_count}\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, sdf_cfg, sdf_cfg_count)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"2ad49d0d-fc5c-4f9c-b2bd-fe25086428a4"},{"cell_type":"markdown","source":["## Get columns"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8187e4f4-f122-4b41-9be8-610d4a18a835"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_get_json_column(parent_column_name):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            sql_code = f\"\"\"SELECT *\n","FROM tvw_cfg\n","WHERE `parent_column_name` = \\\"{parent_column_name}\\\"\n","ORDER BY `sequence`;\"\"\"\n","\n","            rv               = fn_execute_spark_sql(sql_code)\n","            sdf_column       = rv[2]\n","            sdf_column_count = rv[3]\n","\n","            alert             = \"Success\"\n","            alert_description = f\"Rows (count): {sdf_column_count}\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, sdf_column, sdf_column_count)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"ca741036-e117-4815-b883-b65be72070fb"},{"cell_type":"markdown","source":["## Check if JSON file exists"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"95970645-70a2-433f-9374-38e1b395fac4"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_does_json_file_exist(dict_parameter):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            folder_name = dict_parameter[\"folder_name_clean\"]\n","            if folder_name != \"\": folder_name = f\"{folder_name}/\" # if folder_name is not an empty folder, add slash to the end\n","            file_name = dict_parameter[\"file_name_clean\"]\n","            file_name = f\"{global_parameter.abfs_path_lh_bronze}/Files/incoming/JSON/{folder_name}{file_name}.json\"\n","\n","            if notebookutils.fs.exists(file_name) == True:\n","                does_file_exist   = True\n","                alert             = \"Success\"\n","                alert_description = f\"\"\n","            else:\n","                does_file_exist = False\n","                alert             = \"Warning\"\n","                alert_description = f\"File \\\"{file_name}\\\" does not exist.\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, does_file_exist)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7f3b3b93-4326-4107-a3b7-f98c477e3d15"},{"cell_type":"markdown","source":["## Read JSON file into temp view json_data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1ec9d738-5aa8-41be-8762-a9c25781f35c"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_read_json_file(dict_parameter):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            folder_name = dict_parameter[\"folder_name_clean\"]\n","            if folder_name != \"\": folder_name = f\"{folder_name}/\" # if \"folder_name\" is not an empty folder, add slash to the end\n","\n","            # Delete tvw_json_data if exists\n","            sql_code = f\"DROP VIEW IF EXISTS `tvw_json_data`;\"\n","            fn_execute_spark_sql(sql_code)[2]\n","\n","            sql_code = f\"\"\"CREATE OR REPLACE TEMP VIEW `tvw_json_data` AS \n","SELECT *\n","FROM json.`{global_parameter.abfs_path_lh_bronze}/Files/incoming/JSON/{folder_name}{dict_parameter[\"file_name_clean\"]}.json`;\"\"\"\n","            \n","            fn_execute_spark_sql(sql_code)[2]\n","\n","            alert             = \"Success\"\n","            alert_description = f\"\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","            \n","            return (alert, alert_description)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"be6bda2b-9d4f-41e2-bd81-3b59bd6f52da"},{"cell_type":"markdown","source":["## Create temp view for root array"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"79bedc3d-e039-4511-934b-70ca6f313f67"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_create_tvw_root_array(column_name, does_root_array_exist):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            # Get child columns for this parent\n","            sdf_root_array_column = fn_get_json_column(column_name)[2]    \n","\n","            if does_root_array_exist == True:\n","                sql_code = f\"\"\"/* root array exists */ \n","\"\"\"\n","                sql_code += f\"\"\"CREATE OR REPLACE TEMP VIEW `tvw_{column_name}` AS \n","SELECT \n","    CAST(ROW_NUMBER() OVER(ORDER BY (SELECT NULL)) AS Long) AS `fa_{column_name}_id` /* Own ID */ \"\"\"\n","\n","                sql_code += f\"\"\"\n","    /* Columns, belonging to root array */\"\"\"\n","                for row in sdf_root_array_column\\\n","                    .sort(sdf_root_array_column.sequence)\\\n","                    .filter((sdf_root_array_column.data_type != \"Struct\") & (sdf_root_array_column.data_type != \"Array\"))\\\n","                    .collect():\n","                    sql_code += f\"\"\"\n","    , CAST(`_{column_name}`.`{row.column_name}` AS {row.data_type}) AS `{row.column_name}` \"\"\"\n","\n","                sql_code += f\"\"\"\n","    /* Array and Struct, belonging to root array */ \"\"\"\n","                for row in sdf_root_array_column\\\n","                    .sort(sdf_root_array_column.sequence)\\\n","                    .filter((sdf_root_array_column.data_type == \"Struct\") | (sdf_root_array_column.data_type == \"Array\"))\\\n","                    .collect():\n","                    sql_code += f\"\"\"\n","    , `_{column_name}`.`{row.column_name}` AS `{row.column_name}` /* {row.data_type} */ \"\"\"\n","\n","                sql_code += f\"\"\"\n","FROM `tvw_json_data` \n","LATERAL VIEW EXPLODE_OUTER(`{column_name}`) AS `_{column_name}`;\n","                \"\"\"\n","            else:\n","                sql_code = f\"\"\"/* root array does not exist */ \n","\"\"\"\n","                sql_code += f\"\"\"CREATE OR REPLACE TEMP VIEW `tvw_{column_name}` AS \n","SELECT \n","    * \n","    , CAST(ROW_NUMBER() OVER(ORDER BY (SELECT NULL)) AS Long) AS `fa_{column_name}_id` /* Own ID */ \n","FROM tvw_json_data;\"\"\"\n","\n","            fn_execute_spark_sql(sql_code)\n","\n","            alert             = \"Success\"\n","            alert_description = f\"\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, sql_code)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf_root_array_column\" in locals(): par[\"sdf_root_array_column\"] = sdf_root_array_column.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"74663422-9b21-46f1-b4ef-b2323ce3dea0"},{"cell_type":"markdown","source":["## Create temp view for not root array"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4bb64569-c2a4-42a1-98b2-7213eea95bec"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_create_tvw(\n","        column_name\n","        , parent_column_name\n","        , breadcrumb_column_name\n","        , breadcrumb_parent_column_name\n","        , data_type\n","        , child_list\n","    ):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            # Get child columns for this parent\n","            sdf_column = fn_get_json_column(column_name)[2]\n","            sdf_column_count = sdf_column.count()\n","\n","            if breadcrumb_parent_column_name == \"\": breadcrumb_parent_column_name = parent_column_name\n","\n","            sql_code = f\"\"\"CREATE OR REPLACE TEMP VIEW `tvw_{breadcrumb_column_name}` AS \n","SELECT\"\"\"\n","            \n","            if child_list != \"\":\n","                sql_code += f\"\"\"\n","    CAST(ROW_NUMBER() OVER(ORDER BY (SELECT NULL)) AS Long) AS `fa_{breadcrumb_column_name}_id` /* Own ID */ \n","    , `fa_{breadcrumb_parent_column_name}_id` /* Parent ID */ \"\"\"\n","            else:\n","                sql_code += f\"\"\"\n","    `fa_{breadcrumb_parent_column_name}_id` /* Parent ID */ \"\"\"\n","\n","            if sdf_column_count > 0:\n","                sql_code += f\"\"\"\n","    /* Columns, belonging to \\\"{column_name}\\\" */\"\"\"\n","                for row in sdf_column\\\n","                    .sort(sdf_column.sequence)\\\n","                    .filter((sdf_column.data_type != \"Struct\") & (sdf_column.data_type != \"Array\"))\\\n","                    .collect():\n","                    \n","                    if data_type == \"Array\":\n","                        sql_code += f\"\"\"\n","    , CAST(`_{row.breadcrumb_parent_column_name}`.`{row.column_name}` AS {row.data_type}) AS `{row.breadcrumb_column_name}` \"\"\"\n","                    else:\n","                        sql_code += f\"\"\"\n","    , CAST(`{row.breadcrumb_parent_column_name}`.`{row.column_name}` AS {row.data_type}) AS `{row.breadcrumb_column_name}` \"\"\"\n","                \n","                sql_code += f\"\"\"\n","    /* Array and Struct, belonging to \\\"{column_name}\\\" */ \"\"\"\n","                for row in sdf_column\\\n","                    .sort(sdf_column.sequence)\\\n","                    .filter((sdf_column.data_type == \"Struct\") | (sdf_column.data_type == \"Array\"))\\\n","                    .collect():\n","                    if data_type == \"Array\":\n","                        sql_code += f\"\"\"\n","    , `_{row.breadcrumb_parent_column_name}`.`{row.column_name}` AS `{row.breadcrumb_column_name}` /* {row.data_type} */ \"\"\"\n","                    else:\n","                        sql_code += f\"\"\"\n","    , `{row.breadcrumb_parent_column_name}`.`{row.column_name}` AS `{row.breadcrumb_column_name}` /* {row.data_type} */ \"\"\"\n","            else:\n","                sql_code += f\"\"\"\n","    , `_{breadcrumb_column_name}` \"\"\"\n","\n","            sql_code += f\"\"\"\n","FROM `tvw_{breadcrumb_parent_column_name}` \"\"\"\n","\n","            if data_type == \"Array\":\n","                sql_code += f\"\"\"\n","LATERAL VIEW EXPLODE_OUTER(`{breadcrumb_column_name}`) as `_{breadcrumb_column_name}` /* {data_type} */ \"\"\"\n","\n","            sql_code += \";\"\n","\n","            fn_execute_spark_sql(sql_code)[2]\n","\n","            alert             = \"Success\"\n","            alert_description = f\"\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, sql_code)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf_column\" in locals(): par[\"sdf_column\"] = sdf_column.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"06b22bdd-e6e1-4944-b27e-554a9abe9732"},{"cell_type":"markdown","source":["## Get a list of the JSON files to extract"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"59bdfa6b-d105-4ee0-9907-c0b765eecdfa"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_get_extract_list(\n","        technology\n","        , frequency\n","    ):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            sql_code = f\"\"\"SELECT DISTINCT\n","    `technology`\n","    , `frequency`\n","    , IFNULL(`folder_name`, \\\"\\\") AS `folder_name`\n","    , `file_name`\n","FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_json`\n","WHERE\n","    `technology`       = \\\"{technology}\\\"\n","    AND `frequency`    = \\\"{frequency}\\\"\n","    AND `is_extracted` = 1;\"\"\"\n","            sdf_extract = fn_execute_spark_sql(sql_code)[2]\n","            sdf_extract_count = sdf_extract.count()\n","\n","            alert             = \"Success\"\n","            alert_description = f\"Rows (count): {sdf_extract_count}\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, sdf_extract, sdf_extract_count)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf_extract\" in locals(): par[\"sdf_extract\"] = sdf_extract.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f25de02b-86bf-409d-80b3-d44246a3bcc7"},{"cell_type":"markdown","source":["## Create dynamic code to join the temp views"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b873b448-a250-4370-907a-b190a58537ea"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_get_sql_code_join_tvw():\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            # Generate SELECT statement\n","            sql_code = f\"\"\"SELECT\n","    `sequence`\n","    , IF (\n","        `breadcrumb_parent_column_name` = ''\n","        , CONCAT(`parent_column_name`, '`.`', `breadcrumb_column_name`)\n","        , `breadcrumb_column_name`\n","    ) AS `column_name`\n","FROM `tvw_cfg`\n","WHERE\n","    data_type != 'Array'\n","    AND `column_name` NOT IN\n","    (\n","        SELECT DISTINCT `parent_column_name`\n","        FROM `tvw_cfg`\n","    )\n","\n","UNION ALL SELECT\n","    `sequence`\n","    , CONCAT('_', `breadcrumb_column_name`) AS `column_name`\n","FROM `tvw_cfg`\n","WHERE\n","    data_type = 'Array'\n","    AND column_name NOT IN\n","    (\n","        SELECT DISTINCT `parent_column_name`\n","        FROM `tvw_cfg`\n","    )\n","ORDER BY `sequence`;\"\"\"\n","            sdf = fn_execute_spark_sql(sql_code)[2]\n","\n","            list_select = str([row.column_name for row in sdf.select('column_name').collect()]).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"`\")\n","            sql_code_select = f\"\"\"SELECT DISTINCT {list_select}\"\"\"\n","\n","            # Generate FROM statement\n","            sql_code = f\"\"\"SELECT\n","    `sequence`\n","    , CONCAT(\n","        CONCAT('`tvw_', column_name, '`')\n","        , ' AS '\n","        , CONCAT('`', column_name, '`')\n","    ) AS statement_from\n","FROM\n","    (\n","        SELECT\n","            ROW_NUMBER() OVER (ORDER BY `sequence`) AS R\n","            , `sequence`\n","            , column_name\n","        FROM `tvw_cfg`\n","        WHERE data_type IN ('Array', 'Struct')\n","    ) AS A\n","WHERE A.R = 1\n","\n","UNION ALL SELECT\n","    `sequence`\n","    , CONCAT(\n","        ' LEFT JOIN '\n","        , CONCAT('`tvw_', IF(\n","            breadcrumb_column_name = ''\n","            , column_name\n","            , breadcrumb_column_name), '`'\n","        )\n","        , ' AS '\n","        , IF(\n","            breadcrumb_column_name = ''\n","            , CONCAT('`', column_name, '`')\n","            , CONCAT('`', breadcrumb_column_name, '`')\n","        )    \n","        , ' ON '    \n","        , IF(\n","            breadcrumb_parent_column_name = ''\n","            , CONCAT('`', parent_column_name, '`')\n","            , CONCAT('`', breadcrumb_parent_column_name, '`')\n","        )        \n","        , '.'\n","        , IF(\n","            breadcrumb_parent_column_name = ''\n","            , CONCAT('`fa_', parent_column_name, '_id`')\n","            , CONCAT('`fa_', breadcrumb_parent_column_name, '_id`')\n","        )\n","        , ' = '    \n","        , CONCAT('`', breadcrumb_column_name, '`')\n","        , '.'\n","        , IF(\n","            breadcrumb_parent_column_name = ''\n","            , CONCAT('`fa_', parent_column_name, '_id`')\n","            , CONCAT('`fa_', breadcrumb_parent_column_name, '_id`')\n","        )\n","    ) AS statement_from\n","FROM\n","    (\n","        SELECT\n","            ROW_NUMBER() OVER (ORDER BY `sequence`) AS R\n","            , *\n","        FROM `tvw_cfg`\n","        WHERE data_type IN ('Array', 'Struct')\n","    ) AS A\n","WHERE A.R > 1\n","ORDER BY `sequence`;\"\"\"\n","            sdf = fn_execute_spark_sql(sql_code)[2]\n","\n","            list_from = [row.statement_from for row in sdf.select('statement_from').collect()]\n","\n","            statement_from = \"\"\n","            for row in list_from: statement_from += row\n","            sql_code_from = f\"\"\"FROM {statement_from}\"\"\"\n","\n","            # Generate the final statement\n","            sql_code_join = f\"{sql_code_select}\\n{sql_code_from};\"\n","\n","            alert             = \"Success\"\n","            alert_description = f\"\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, sql_code_join)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8870014c-d458-4a9e-8c97-878d83ab7f65"},{"cell_type":"markdown","source":["## Clean up column names"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"44414d3b-2764-465c-a0d8-6109e485acb0"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_clean_column_name_in_sdf_final(sdf_final):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            # Get column names\n","            list_column = sdf_final.columns\n","            \n","            # Create dataframe with column names\n","            sdf_column = sqlContext.createDataFrame([(l,) for l in list_column], [\"column_name\"])\n","            \n","            # Create temp view with column names\n","            sdf_column.createOrReplaceTempView(\"tvw_column\")\n","            \n","            # Replace first underscore from column name\n","            sql_code = f\"\"\"SELECT\n","    IF(LEFT(`column_name`, 1) = '_', SUBSTRING(`column_name`, 2, LEN(`column_name`)), `column_name`) AS `column_name`\n","    , IF(LEFT(`column_name`, 1) = '_', SUBSTRING(`column_name`, 2, LEN(`column_name`)), `column_name`) AS `column_name_clean`\n","FROM `tvw_column`;\"\"\"\n","            \n","            # Create dataframe with column names\n","            sdf_column = fn_execute_spark_sql(sql_code)[2]\n","            \n","            # Delete the temp view\n","            fn_execute_spark_sql(\"DROP VIEW IF EXISTS tvw_column;\")[2]\n","\n","            # Clean non alphanumericals\n","            sdf_column = fn_replace_all_not_alphanumericals_in_single_column_in_sdf(\n","                sdf_column,\n","                \"column_name_clean\",\n","                \"_\"\n","            )[2]\n","\n","            # Clean accents\n","            sdf_column = sdf_column.withColumn(\"column_name_clean\", fn_clean_accent_in_sdf_column(\"column_name_clean\")[2])\n","\n","            # Convert to pdf\n","            pdf_column = sdf_column.toPandas()\n","\n","            column_list = pdf_column[\"column_name_clean\"].to_list()\n","\n","            # Rename all the columns\n","            sdf_final = sdf_final.toDF(*column_list)\n","\n","            # Count the rows in sdf_final\n","            sdf_final_count = sdf_final.count()\n","\n","            alert             = \"Success\"\n","            alert_description = f\"Rows (count): {sdf_final_count}\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, sdf_final, sdf_final_count)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf_final\" in locals(): par[\"sdf_final\"] = sdf_final.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"edcaea7e-294f-4557-b25e-53e707890219"},{"cell_type":"markdown","source":["## Save the result in bronze table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"28bc6553-ee5a-4128-a392-cc1fbc7f923f"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_save_json_as_table(dict_parameter, sdf):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            technology  = dict_parameter[\"technology_clean\"]\n","            folder_name = dict_parameter[\"folder_name_clean\"]\n","            file_name   = dict_parameter[\"file_name_clean\"]\n","\n","            # Generate full table path\n","            file_name = f\"{global_parameter.abfs_path_lh_bronze}/Tables/{technology}_{folder_name}_{file_name}\"\n","\n","            # Remove double underscore (folder_name is empty)\n","            file_name = file_name.replace(f\"__\", \"_\")\n","\n","            # Save the extracted JSON file as delta table in lh_bronze\n","            fn_save_sdf_as_table(\n","                sdf\n","                , \"delta\"\n","                , \"overwrite\"\n","                , file_name\n","                , dict_parameter\n","            )\n","\n","            alert             = \"Success\"\n","            alert_description = f\"\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, file_name)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"a8a37679-f1fd-45e7-86bd-5c40e3a8710d"},{"cell_type":"markdown","source":["## Drop all temp views"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6fd577f9-c876-44ba-9c4d-f0cf4f7e6bdd"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_drop_all_temp_views():\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            sdf = fn_execute_spark_sql(f\"SHOW VIEWS;\")[2]\n","\n","            for row in sdf.filter(sdf.isTemporary == True).collect():\n","                sql_code = f\"DROP VIEW IF EXISTS `{row.viewName}`;\"\n","                if is_debug: print(f\"sql_code: {sql_code}\")\n","                fn_execute_spark_sql(sql_code)[2]\n","\n","            alert             = \"Success\"\n","            alert_description = f\"\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"369af919-0127-4e0c-8ebd-4b9b9dfb0695"},{"cell_type":"markdown","source":["# Operations"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"59eab224-9f07-418c-81ea-6a10dd07c20a"},{"cell_type":"markdown","source":["## Extract JSON"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e1926cb9-2b19-4244-bb03-5bb7235d0625"},{"cell_type":"code","source":["if \"JSON\" in list_technology:\n","    def fn_extract_json(technology, frequency):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            # Rename the existing files in lh_bronze\n","            sdf_renamed = fn_rename_files_in_lh_bronze(technology, frequency, \"Archive\")[2]\n","            if is_debug:\n","                print(f\"sdf_renamed\")\n","                display(sdf_renamed)\n","\n","            # Get a list of JSON files to extract\n","            sdf_extract = fn_get_extract_list(technology, frequency)[2]\n","            sdf_extract_count = sdf_extract.count()\n","\n","            if is_debug:\n","                print(f\"sdf_extract\")\n","                display(sdf_extract)\n","                print(f\"sdf_extract_count: {sdf_extract_count}\")\n","\n","            # Loop sdf_extract\n","            for row_e in sdf_extract.collect():\n","                # Set dict_parameter\n","                dict_parameter = {\"technology\": row_e.technology, \"frequency\": row_e.frequency, \"folder_name\": row_e.folder_name, \"file_name\": row_e.file_name}                \n","\n","                # Clean up \"technology\", \"folder_name\", \"file_name\" - Start\n","                # Clean accents and not alphanumerics\n","                technology_clean  = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(technology)[2], \"_\")[2]\n","                folder_name_clean = row_e.folder_name.replace(\"/\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\") # keep \"/\" while cleaning\n","                folder_name_clean = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(folder_name_clean)[2], \"_\")[2]\n","                folder_name_clean = folder_name_clean.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"/\")\n","                file_name_clean   = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(row_e.file_name)[2], \"_\")[2]\n","\n","                dict_parameter[\"technology_clean\"]  = technology_clean\n","                dict_parameter[\"folder_name_clean\"] = folder_name_clean\n","                dict_parameter[\"file_name_clean\"]   = file_name_clean\n","\n","                if is_debug:\n","                    print(f\"folder_name_clean: >{folder_name_clean}<\")\n","                    print(f\"file_name_clean: >{file_name_clean}<\")\n","                    print(f\"dict_parameter: {dict_parameter}\")\n","                # Clean up \"technology\", \"folder_name\", \"file_name\" - End\n","\n","                # Check if the current file exists\n","                rv              = fn_does_json_file_exist(dict_parameter)\n","                does_file_exist = rv[2]\n","                if is_debug: print(f\"does_file_exist: {does_file_exist}\")\n","\n","                if does_file_exist == True:\n","                    # Create tvw_cfg\n","                    fn_create_tvw_cfg(dict_parameter)\n","                    \n","                    # Check if root array exists and get its column_name\n","                    rv = fn_get_root_array_info()    \n","                    does_root_array_exist = rv[2]\n","                    root_array_column_name = rv[3]\n","\n","                    if is_debug:\n","                        print(f\"does_root_array_exist: {does_root_array_exist}\")\n","                        print(f\"root_array_column_name: >{root_array_column_name}<\")\n","\n","                    # Create tvw_parent_child - Start\n","                    rv = fn_create_cfg_parent_child(does_root_array_exist, root_array_column_name)\n","                    sdf_cfg = rv[2]\n","                    sdf_cfg_count = rv[3]\n","                    if is_debug:\n","                        print(\"sdf_cfg:\")\n","                        display(sdf_cfg)\n","                        print(f\"sdf_cfg_count: {sdf_cfg_count}\")\n","\n","                    # Clean up\n","                    fn_execute_spark_sql(\"DROP VIEW IF EXISTS tvw_cfg_parent_child;\")\n","                    fn_execute_spark_sql(\"DROP VIEW IF EXISTS tvw_recursive;\")\n","                    fn_execute_spark_sql(\"DROP VIEW IF EXISTS tvw_final;\")\n","                    # Create tvw_parent_child - End\n","\n","                    # Read the data from the current file (if exists) into tvw_json_data\n","                    fn_read_json_file(dict_parameter)\n","\n","                    # Create temp views for root array and all the nested Array or Struct\n","                    for row_f in sdf_cfg\\\n","                        .filter((sdf_cfg.data_type == \"Struct\") | (sdf_cfg.data_type == \"Array\"))\\\n","                        .sort(sdf_cfg.level, sdf_cfg.sequence)\\\n","                        .collect():\n","\n","                        if row_f.level == 1:\n","                            if is_debug:\n","                                print(f\"row_f:\")\n","                                print(f\"  level: {row_f.level}\")\n","                                print(f\"  sequence: {row_f.sequence}\")\n","                                print(f\"  column_name: {row_f.column_name}\")\n","                                print(f\"  parent_column_name: {row_f.parent_column_name}\")\n","                                print(f\"  breadcrumb_column_name: {row_f.breadcrumb_column_name}\")\n","                                print(f\"  breadcrumb_parent_column_name: {row_f.breadcrumb_parent_column_name}\")\n","                                print(f\"  data_type: {row_f.data_type}\")\n","                                print(f\"  child_list: {row_f.child_list}\")\n","                            \n","                            fn_create_tvw_root_array(\n","                                row_f.column_name\n","                                , does_root_array_exist\n","                            )\n","                        else:\n","                            if is_debug:\n","                                print(f\"row_f:\")\n","                                print(f\"  level: {row_f.level}\")\n","                                print(f\"  sequence: {row_f.sequence}\")\n","                                print(f\"  column_name: {row_f.column_name}\")\n","                                print(f\"  parent_column_name: {row_f.parent_column_name}\")\n","                                print(f\"  breadcrumb_column_name: {row_f.breadcrumb_column_name}\")\n","                                print(f\"  breadcrumb_parent_column_name: {row_f.breadcrumb_parent_column_name}\")\n","                                print(f\"  data_type: {row_f.data_type}\")\n","                                print(f\"  child_list: {row_f.child_list}\")\n","                            \n","                            fn_create_tvw(\n","                                row_f.column_name\n","                                , row_f.parent_column_name\n","                                , row_f.breadcrumb_column_name\n","                                , row_f.breadcrumb_parent_column_name\n","                                , row_f.data_type\n","                                , row_f.child_list\n","                            )\n","                        if is_debug: print(\"---------- Loop level 1, >1 end ----------\")\n","                        \n","                    if is_debug:\n","                        print(\"SHOW VIEWS:\")\n","                        display(spark.sql(\"SHOW VIEWS;\"))\n","\n","                    # Generate SQL code to join the root array to all the nested Array or Struct\n","                    rv            = fn_get_sql_code_join_tvw()\n","                    sql_code_join = rv[2]\n","                    if is_debug: print(f\"sql_code_join: {sql_code_join}\")\n","                    \n","                    # Get the final dataframe\n","                    rv              = fn_execute_spark_sql(sql_code_join)\n","                    sdf_final       = rv[2]\n","                    sdf_final_count = rv[3]\n","\n","                    # Clean up all the columns names in the final dataframe (delta table does not accept accents and non aplhanumericals in the column names)\n","                    sdf_final = fn_clean_column_name_in_sdf_final(sdf_final)[2]\n","                    if is_debug:\n","                        print(f\"sdf_final:\")\n","                        display(sdf_final)\n","                        print(f\"sdf_final_count: {sdf_final_count}\")\n","\n","                    # Save JSON file as table in lh_bronze\n","                    fn_save_json_as_table(dict_parameter, sdf_final)\n","\n","                    # Copy and delete JSON file to \"lh_bronze/Processed/JSON\" (archive) - Start\n","                    # Set \"src_file_path\" and \"dstn_file_path\"\n","                    now_year  = fn_get_now(\"year\")[1]\n","                    now_month = fn_get_now(\"month\")[1]\n","                    src_file_path  = f\"{global_parameter.abfs_path_lh_bronze}/Files/incoming/{technology_clean}/{folder_name_clean}/{file_name_clean}.json\"\n","                    dstn_file_path = f\"{global_parameter.abfs_path_lh_bronze}/Files/processed/{technology_clean}/{folder_name_clean}/{file_name_clean}/\\\n","{now_year}/{now_month}/\\\n","{file_name_clean}_{global_parameter.process_timestamp}.json\"\n","\n","                    # Remove double slash from source and destination file paths (\"folder_name\" is empty)\n","                    src_file_path  = src_file_path.replace(\"abfss://\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n","                    src_file_path  = src_file_path.replace(\"//\", \"/\")\n","                    src_file_path  = src_file_path.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"abfss://\")\n","                    dstn_file_path = dstn_file_path.replace(\"abfss://\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n","                    dstn_file_path = dstn_file_path.replace(\"//\", \"/\")\n","                    dstn_file_path = dstn_file_path.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"abfss://\")\n","\n","                    # Copy and delete\n","                    copy_file_status = fn_copy_file(src_file_path, dstn_file_path, dict_parameter)[0]\n","                    if copy_file_status == \"Success\": fn_delete_file(src_file_path, dict_parameter)\n","\n","                    if is_debug:\n","                        print(f\"src_file_path:  {src_file_path}\")\n","                        print(f\"dstn_file_path: {dstn_file_path}\")            \n","                    # Copy and delete JSON file to \"lh_bronze/Processed/JSON\" (archive) - End\n","\n","                    # Clean up (drop all temp views)\n","                    fn_drop_all_temp_views()\n","\n","                    # Insert \"Success\" in lh_log (for each file) - Start\n","                    # Always log \"Success\" for extracted objects.\n","                    alert             = \"Success\"\n","                    alert_description = f\"Row (count): {sdf_final_count}\"\n","\n","                    if is_debug: del par\n","                    fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)\n","                    # Insert \"Success\" in lh_log (for each file) - End\n","\n","                    if is_debug: print(\"------------------------- Loop end -------------------------\")\n","\n","            alert             = \"Success\"\n","            alert_description = \"\"\n","\n","            return (alert, alert_description)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description)\n","\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","            \n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f98323c6-ad22-4beb-8c2f-49faf2b60d8d"},{"cell_type":"code","source":["if is_debug: print(\"\\n---------- This notenook name: nb_bronze_function_extract_json - End ----------\\n\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c1103ec3-b2b2-4412-a9b0-8fa982157a2b"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{}},"nbformat":4,"nbformat_minor":5}