{"cells":[{"cell_type":"markdown","source":["# System"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"43790397-e807-4a59-a54c-34c24530a64e"},{"cell_type":"code","source":["if is_debug: print(\"\\n---------- This notenook name: nb_bronze_function_extract_excel - Start ----------\\n\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d87c613a-2e3c-40d2-bb83-709f33f785cf"},{"cell_type":"code","source":["medallion_name = \"Bronze\"\n","if is_debug: print(f\"medallion_name: {medallion_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9cd1ac2c-ad91-4070-83d1-9a7e89a31d9c"},{"cell_type":"code","source":["# Current session info\n","if is_debug: display(ss.getActiveSession())"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bbf2debe-a286-4fd7-92a0-2e322192dae4"},{"cell_type":"markdown","source":["# Function"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"60d55c01-0a5d-4af3-9642-ff7285394be2"},{"cell_type":"markdown","source":["## Get the existing excel files in \"lh_bronze\""],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d0d9f34f-22cd-4304-867d-259e939c72e3"},{"cell_type":"code","source":["# Will be joined to datasets \"archive table list\" and \"extract table list\" to exclude the non-existing files in \"lh_bronze\" from the loops\n","\n","if \"Excel\" in list_technology:\n","    def fn_get_excel_files_in_lh_bronze(\n","        excel_folder_name\n","        , list_frequency\n","    ):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            # Create empty dataframe to keep the existing files in the lakehouse\n","            schema_exist = st.StructType([\n","                st.StructField('folder_name_original', st.StringType())\n","                , st.StructField('file_name_original', st.StringType())\n","                , st.StructField('folder_name', st.StringType())\n","                , st.StructField('file_name', st.StringType())\n","            ])\n","            sdf_exist = spark.createDataFrame([], schema_exist)\n","\n","            # Collect \"folder_name\", \"file_name\"\n","            sql_code = f\"\"\"SELECT DISTINCT\n","    `technology`\n","    , `frequency`\n","    , IFNULL(`folder_name`, \\\"\\\") AS `folder_name`\n","    , `file_name`\n","FROM delta.`{global_parameter.abfs_path_lh_cfg}/Tables/extract_object_excel`\n","WHERE\n","    `technology`       = \\\"Excel\\\"\n","    AND `frequency`    IN ({str(list_frequency).replace(\"[\", \"\").replace(\"]\", \"\")})\n","    AND `is_extracted` = 1;\"\"\"\n","            sdf_folder_file = fn_execute_spark_sql(sql_code)[2]            \n","\n","            for row in sdf_folder_file.collect():\n","                dict_parameter = {\"technology\": row.technology, \"frequency\": row.frequency, \"folder_name\": row.folder_name, \"file_name\": row.file_name}\n","                folder_name_clean = row.folder_name.replace(\"/\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\") # keep \"/\" while cleaning\n","                folder_name_clean = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(folder_name_clean)[2], \"_\")[2]\n","                folder_name_clean = folder_name_clean.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"/\")\n","                file_name_clean   = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(row.file_name)[2], \"_\")[2]\n","                sdf_file          = fn_list_folder(\n","                    f\"{global_parameter.abfs_path_lh_bronze}/Files/incoming/{excel_folder_name}/{folder_name_clean}/{file_name_clean}.xlsx\"\n","                    , dict_parameter\n","                )\n","\n","                for r in sdf_file[2]:\n","                    new_row = spark.createDataFrame([(row.folder_name, row.file_name, folder_name_clean, r.name)], schema_exist)\n","                    sdf_exist = sdf_exist.union(new_row)\n","\n","                sdf_exist = sdf_exist.withColumn(\"file_name\", sf.regexp_replace(\"file_name\", \".xlsx\", \"\"))\n","            sdf_exist_count = sdf_exist.count()\n","\n","            alert             = \"Success\"\n","            alert_description = f\"Existing excel files (count): {sdf_exist_count}\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, sdf_exist, sdf_exist_count)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","        fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"a7f40f4d-4494-45a3-80a5-e0fc925f210f"},{"cell_type":"markdown","source":["## Read Excel file"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"599523c0-206d-41dd-a609-61973a7ea911"},{"cell_type":"code","source":["if \"Excel\" in list_technology:\n","    def fn_read_excel_file(\n","        file_name\n","        , sheet_name\n","        , first_row\n","        , dict_parameter\n","    ):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            pdf = pd.read_excel(\n","                io = file_name\n","                , sheet_name = sheet_name\n","                , header = first_row\n","                , dtype = str\n","            )\n","            pdf_count = len(pdf.index)\n","\n","            alert             = \"Success\"\n","            alert_description = f\"Excel file (rows): {pdf_count}\"\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description, pdf, pdf_count)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","\n","            return (alert, alert_description, None, None)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)\n","                del par\n","\n","            pdf = pdf.head(5)\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"12746252-a209-48f7-978c-58b5381150af"},{"cell_type":"markdown","source":["# Operation"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5ff6a960-6df2-41ed-a229-f356866fa9ff"},{"cell_type":"markdown","source":["## Extract Ecel"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"300b7c40-6d39-4b73-bac4-77590afc6add"},{"cell_type":"code","source":["if \"Excel\" in list_technology:\n","    def fn_extract_excel(\n","        technology\n","        , frequency\n","    ):\n","        fn_name        = stk()[0][3]\n","        caller_fn_name = stk()[1].function.replace(\"<module>\", \"\")\n","        if is_debug: par = {}\n","\n","        try:\n","            # Rename the existing files in \"lh_bronze\"\n","            fn_rename_files_in_lh_bronze(technology, frequency, \"Archive\")\n","\n","            # Get the existing excel files in \"lh_bronze\"\n","            sdf_exist = fn_get_excel_files_in_lh_bronze(technology, list_frequency)[2]\n","            if is_debug:\n","                print(f\"sdf_exist:\")\n","                display(sdf_exist)\n","\n","            # Get extract table list\n","            dict_parameter = {\"technology\": technology, \"frequency\": frequency, \"action\": \"Extract\"}\n","            sdf_extract = fn_get_extract_table_list(dict_parameter)[2]\n","\n","            # Join \"existing\" and \"extract\" files to remove the not existing from the \"loop\" list\n","            sdf_extract = sdf_extract.alias(\"x\")\\\n","                .join(\n","                    sdf_exist.alias(\"e\")\n","                    , [\n","                        sf.col(\"x.folder_name\") == sf.col(\"e.folder_name_original\")\n","                        , sf.col(\"x.file_name\") == sf.col(\"e.file_name_original\")\n","                    ]\n","                    , \"inner\")\\\n","                .select(sf.col(\"x.technology\"), sf.col(\"x.frequency\"), sf.col(\"x.folder_name\")\n","                    , sf.col(\"x.file_name\"), sf.col(\"x.worksheet_name\"), sf.col(\"x.first_row\"))\n","            if is_debug:\n","                print(f\"sdf_extract:\")\n","                display(sdf_extract)\n","\n","            # Loop extract list\n","            for row_e in sdf_extract.collect():\n","                # Clean up \"folder_name\", \"file_name\"\n","                folder_name_clean = row_e.folder_name.replace(\"/\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\") # keep \"/\" while cleaning\n","                folder_name_clean = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(folder_name_clean)[2], \"_\")[2]\n","                folder_name_clean = folder_name_clean.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"/\")    \n","                folder_name_clean = fn_clean_accent_in_string(folder_name_clean)[2]\n","                file_name_clean   = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(row_e.file_name)[2], \"_\")[2]\n","                #file_name_clean   = fn_clean_accent_in_string(file_name_clean)[2] ???\n","                \n","                # Prepare the parameters for \"fn_read_excel_file()\"\n","                excel_file_path      = f\"{global_parameter.abfs_path_lh_bronze}/Files/incoming/Excel/{folder_name_clean}/{file_name_clean}.xlsx\"\n","                sheet_name     = row_e.worksheet_name\n","                first_row      = int(row_e.first_row)\n","                dict_parameter = {\"technology\": row_e.technology\n",", \"frequency\": row_e.frequency\n",", \"folder_name\": row_e.folder_name\n",", \"folder_name_clean\": folder_name_clean\n",", \"file_name\": row_e.file_name\n",", \"file_name_clean\": file_name_clean\n",", \"worksheet_name\": row_e.worksheet_name}\n","                if is_debug:\n","                    print(f\"file_path: {excel_file_path}\")\n","                    print(f\"sheet_name: {sheet_name}\")\n","                    print(f\"first_row: {first_row}\")\n","                    print(f\"dict_parameter: {dict_parameter}\")\n","\n","                # Generate column list for the current worksheet from table \"lh_cfg.extract_object_excel\"    \n","                sdf_column = fn_get_extract_column(dict_parameter)[2]\n","                sdf_column = sdf_column.withColumn(\"alias_name\", sf.trim(fn_clean_accent_in_sdf_column(\"column_name\")[2]))\n","                sdf_column = fn_replace_all_not_alphanumericals_in_single_column_in_sdf(sdf_column, \"alias_name\", \"_\")[2]\n","                if is_debug:\n","                    print(\"sdf_column:\")\n","                    display(sdf_column)\n","                    \n","                # Extract to Pandas dataframe\n","                pdf_extracted = fn_read_excel_file(excel_file_path, sheet_name, first_row, dict_parameter)[2]\n","                if is_debug:\n","                    print(\"pdf_extracted:\")\n","                    pdf_extracted.info(verbose = True)\n","                    display(pdf_extracted)\n","\n","                # Convert to spark dataframe\n","                sdf_extracted = spark.createDataFrame(pdf_extracted)\n","                sdf_extracted_count = sdf_extracted.count()\n","\n","                # Replace NaN and empty string with NULL in all the columns of the dataframe\n","                sdf_extracted = fn_replace_in_all_columns_in_sdf(sdf_extracted, \"NaN\", None)[2]\n","                sdf_extracted = fn_replace_in_all_columns_in_sdf(sdf_extracted, \"\", None)[2]\n","                sdf_extracted = eval(fn_get_select_column(\"sdf_extracted\", sdf_column)[2])\n","                if is_debug:\n","                    print(\"sdf_extracted:\")\n","                    sdf_extracted.printSchema()        \n","                    display(sdf_extracted)\n","\n","                # Save extracted file as delta table in lh_bronze - Start\n","                # Replace \"/\" with \"_\" in \"folder_name\"\n","                folder_name_clean = folder_name_clean.replace(\"/\", \"_\")\n","\n","                # Clean up \"excel_table_path\" components\n","                technology_clean     = fn_clean_accent_in_string(row_e.technology)[2]\n","                technology_clean     = fn_clean_not_alphanumeric_in_string(technology_clean, \"_\")[2]\n","                worksheet_name_clean = fn_clean_accent_in_string(row_e.worksheet_name)[2]\n","                worksheet_name_clean = fn_clean_not_alphanumeric_in_string(worksheet_name_clean, \"_\")[2]\n","                folder_name_clean    = fn_clean_accent_in_string(folder_name_clean)[2]\n","                folder_name_clean    = fn_clean_not_alphanumeric_in_string(folder_name_clean, \"_\")[2]\n","                file_name_clean      = fn_clean_accent_in_string(row_e.file_name)[2]\n","                file_name_clean      = fn_clean_not_alphanumeric_in_string(file_name_clean, \"_\")[2]                \n","                excel_table_path     = f\"{global_parameter.abfs_path_lh_bronze}/Tables/{technology_clean}_{folder_name_clean}_{file_name_clean}_{worksheet_name_clean}\"\n","                \n","                dict_parameter[\"technology_clean\"]     = technology_clean\n","                dict_parameter[\"folder_name_clean\"]    = folder_name_clean\n","                dict_parameter[\"file_name_clean\"]      = file_name_clean\n","                dict_parameter[\"worksheet_name_clean\"] = worksheet_name_clean\n","\n","                # Remove double underscore (folder_name is empty)\n","                excel_table_path = excel_table_path.replace(f\"__\", \"_\")\n","\n","                fn_save_sdf_as_table(\n","                    sdf_extracted\n","                    , \"delta\"\n","                    , \"overwrite\"\n","                    , excel_table_path\n","                    , dict_parameter\n","                )\n","                # Save extracted file as delta table in lh_bronze - End\n","\n","                # Insert \"Success\" in lh_log (for each file) - Start\n","                alert             = \"Success\"\n","                alert_description = f\"Row count: {sdf_extracted_count}\"\n","\n","                pdf_extracted = pdf_extracted.head(5)\n","\n","                fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)\n","                # Insert \"Success\" in lh_log (for each file) - End\n","\n","                if is_debug: print(\"\\n-------------------- Loop Extract End --------------------\\n\")\n","\n","            # Copy to lh_bronze/Files/processed (archive) - Start\n","            # Create sdf_archive - Start\n","\n","            # Get archive table list\n","            dict_parameter = {\"technology\": technology, \"frequency\": frequency, \"action\": \"Archive\"}\n","            sdf_archive = fn_get_extract_table_list(dict_parameter)[2]\n","\n","            # Join \"existing\" and \"archive\" files to remove the not existing from the \"loop\" list\n","            sdf_archive = sdf_archive.alias(\"a\")\\\n","                .join(\n","                    sdf_exist.alias(\"e\")\n","                    , [\n","                        sf.col(\"a.folder_name\") == sf.col(\"e.folder_name_original\")\n","                        , sf.col(\"a.file_name\") == sf.col(\"e.file_name_original\")\n","                    ]\n","                    , \"inner\")\\\n","                .select(sf.col(\"a.technology\"), sf.col(\"a.frequency\"), sf.col(\"a.folder_name\"), sf.col(\"a.file_name\"))\n","            if is_debug:\n","                print(f\"sdf_archive:\")\n","                display(sdf_archive)\n","            # Create sdf_archive - End\n","\n","            # Prepare dstn path components\n","            now_year = fn_get_now(\"year\")[1]\n","            now_month = fn_get_now(\"month\")[1]\n","\n","            # Loop sdf_archive\n","            for row_a in sdf_archive \\\n","                .sort(\n","                    sdf_archive.technology.asc()\n","                    , sdf_archive.frequency.asc()\n","                    , sdf_archive.folder_name.asc()\n","                    , sdf_archive.file_name.asc()\n","                ) \\\n","                .collect():\n","\n","                # Clean up \"sdf_csv_file_table_path\" components\n","                technology_clean  = fn_clean_accent_in_string(row_a.technology)[2]\n","                technology_clean  = fn_clean_not_alphanumeric_in_string(technology_clean, \"_\")[2]\n","                folder_name_clean = row_a.folder_name.replace(\"/\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\") # keep \"/\" while cleaning\n","                folder_name_clean = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(folder_name_clean)[2], \"_\")[2]\n","                folder_name_clean = folder_name_clean.replace(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", \"/\")    \n","                folder_name_clean = fn_clean_accent_in_string(folder_name_clean)[2]\n","                file_name_clean   = fn_clean_not_alphanumeric_in_string(fn_clean_accent_in_string(row_a.file_name)[2], \"_\")[2]\n","                file_name_clean   = fn_clean_accent_in_string(file_name_clean)[2]\n","\n","                # Add trailing slash if \"folder_name\" is empty\n","                if folder_name_clean != \"\": folder_name_clean = f\"{folder_name_clean}/\"\n","                else:                       folder_name_clean = folder_name_clean\n","\n","                # Generate source and destination file paths            \n","                src  = f\"{global_parameter.abfs_path_lh_bronze}/Files/incoming/Excel/{folder_name_clean}{file_name_clean}.xlsx\"\n","                dstn = f\"{global_parameter.abfs_path_lh_bronze}/Files/processed/Excel/{folder_name_clean}{file_name_clean}/{now_year}/{now_month}/{file_name_clean}_{global_parameter.process_timestamp}.xlsx\"\n","                if is_debug:\n","                    print(f\"copy/delete file (src): {src}\")\n","                    print(f\"copy file (dstn): {dstn}\")\n","\n","                # Copy excel file to \"processed\"\n","                dict_parameter = {\"technology\": row_a.technology\n",", \"technology_clean\": technology_clean\n",", \"frequency\": row_a.frequency\n",", \"folder_name\": row_a.folder_name\n",", \"folder_name_clean\": folder_name_clean\n",", \"file_name\": row_a.file_name\n",", \"file_name_clean\": file_name_clean}\n","                result_copy = fn_copy_file(src, dstn, dict_parameter)[0]                \n","                if is_debug:\n","                    print(f\"dict_parameter: {dict_parameter}\")\n","                    print(f\"result_copy: {result_copy}\")\n","\n","                # Delete excel file from \"incoming\"\n","                if result_copy == \"Success\": fn_delete_file(src, dict_parameter)\n","\n","                if is_debug: print(\"-------------------- Loop Archive End --------------------\")\n","            # Copy to lh_bronze/Files/processed (archive) - End\n","\n","            # fn_print_debug_info_123(\"Success\", fn_name, par) # test \"except\"\n","\n","            return (alert, alert_description)\n","        except Exception as ex:\n","            alert             = \"Danger\"\n","            alert_description = str(ex)\n","            \n","            pdf_extracted = pdf_extracted.head(5)\n","            fn_local_log_insert(global_parameter.process_timestamp, medallion_name, fn_name, fn_locals_to_json(locals())[2], alert, alert_description)\n","            \n","            return (alert, alert_description)\n","        finally:\n","            if is_debug:\n","                par[\"locals\"] = locals()\n","                if \"sdf\" in locals(): par[\"sdf\"] = sdf.show(n = 5)\n","                fn_print_debug_info(alert, fn_name, par)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"1a0c6ab6-ee18-422a-8e43-76464782aec2"},{"cell_type":"code","source":["if is_debug: print(\"\\n---------- This notenook name: nb_bronze_function_extract_excel - End ----------\\n\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4ac28e41-ffa4-40f2-a97e-bad5ae5f6463"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{}},"nbformat":4,"nbformat_minor":5}